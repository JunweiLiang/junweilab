<html>
<head>
	<meta  http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=1" />
	<link rel="stylesheet" type="text/css" href="utils/bootstrap.min.css"/>
  <script language="javascript" src="utils/jquery.min.js"></script>
	<script language="javascript" src="utils/bootstrap.min.js"></script>
  <!-- -->
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css"/>
	<link rel="stylesheet" type="text/css" href="utils/cssReset.css"/>
  <link rel="stylesheet" type="text/css" href="utils/css_layout.css"/>
	<title>Precognition Lab at HKUST (Guangzhou)</title>
  <!-- 搜索引擎优化stuff -->
	<meta name="description"
    content="Academic website for Prof. Junwei Liang's lab, the Precognition Lab. Dr. Junwei Liang is currently a tenure-track Assistant Professor at The Hong Kong University of Science and Technology (Guangzhou). He is also an affiliate assistant professor at HKUST computer science & engineering department. He was a senior researcher at Tencent Youtu Lab working on cutting-edge computer vision research and applications. Prior to that, he received his Ph.D. degree from Carnegie Mellon University, working with Prof. Alexander Hauptmann. He is the recipient of Baidu Scholarship and Yahoo Fellowship, and awarded Rising Star Award at the World AI Conference in 2020. He is the winner of several public safety video analysis competitions, including ASAPS and TRECVID ActEV. His work has helped and been reported by major news agencies like the Washington Post and New York Times. His research interests include human trajectory forecasting, action recognition, and large-scale computer vision and video analytics in general. His mission: develop AI technologies for social good.">
	<meta name="keywords" content="Junwei Liang,CMU,HKUST,HKUST-GZ,Professor,computer vision,EmbodiedAI,PhD,梁俊卫,Carnegie Mellon University,The Hong Kong University of Science and Technology">

	<!-- Global site tag (gtag.js) - Google Analytics/ no use after 06/2024 -->
  <!--
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156016426-1"></script>
  <script>
  	// for Google Analytics, for free!
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-156016426-1');

  </script>
-->

</head>
<body>
<div id="sidebar">
  <!--<img class='me' src="resources/me.jpeg"></img>-->
  <img class='me' src="resources/precognition_lab.png"></img>
  <br/>
  <div class="info">
    <h2 class="name">Lab Director:</h2>
    <h2 class="name">Prof. Junwei Liang</h2>
    <h2 class="name_chinese">梁俊卫</h2>
    <h2 class="email">HKUST (Guangzhou) / HKUST</h2>
    <h2 class="email">Office: E4-304</h2>
    <h2 class="link">
      <a style="font-size: 14px; color: yellow; font-weight: bold;" href="https://junweiliang.me">Personal Page</a>
    </h2>
  <!--
    <h2 class="link">
      <a href="https://www.semanticscholar.org/author/Junwei-Liang/1915796">[Semantic Scholar]</a>
      <a href="https://www.researchgate.net/profile/Junwei_Liang3">[Research Gate]</a>
    </h2>
    <h2 class="link">
      <a href="https://github.com/JunweiLiang">[Github]</a>
      <a href="https://www.linkedin.com/in/junweiliang/">[LinkedIn]</a>
      <a href="https://www.youtube.com/channel/UC-z7ZWp8Rbu2xhxnbAL_bRQ">[Youtube]</a>
    </h2>
    <h2 class="link">
      <a href="https://www.zhihu.com/people/junwei-liang-50">[知乎]</a>
      <a href="https://www.xiaohongshu.com/user/profile/62c3a783000000001b02b099">[小红书]</a>
      <a href="https://twitter.com/JunweilLiang">[Twitter]</a>
    </h2>
  -->

  </div>
  <div id="navigation">
    <a class="nav_item" href="./index.html">
      <i class="icon icon-home icon-white"></i> &nbsp; About
    </a>
    <a class="nav_item" href="./index.html#publications">
      <i class="icon icon-th-large icon-white icon-list"></i> &nbsp; Publications
    </a>
    <a class="nav_item" href="./projects.html#projects">
      <i class="icon icon-th-large icon-white"></i> &nbsp; Projects
    </a>
    <a class="nav_item" href="./index.html#members">
      <i class="icon icon-white icon-user"></i> &nbsp; Lab Members
    </a>
    <a class="nav_item" href="./internal/index.html">
      <i class="icon icon-book icon-white"></i> &nbsp; Internal Docs
    </a>

  </div>
</div>

<div id="main">
  <div class="title" style="padding:40px 0">
    <a class="title_link" id="bio" href="#bio">Precognition Lab @ HKUST (Guangzhou)</a>
    <!--<img src="https://vbr.wocr.tk/badge?page_id=JunweiLiang.precognition&right_color=green" alt=""/> -->
    <img src="https://vbr.wocr.tk/badge?page_id=JunweiLiang.JunweiLiang&right_color=green" alt=""/>
    <img id="logo" src="resources/hkustgz-logo.jpg"></img>
  </div>


  <div class="content">
    Our research lab, the Precognition Lab (智能感知与预测实验室), is interested in building human-level <span style="font-weight: bold;">Embodied AI</span> systems that can effectively perceive, reason and interact with the real world for the good of humans.
    Here is an up-to-date <a href="./projects.html#roadmap">research roadmap</a>.

    <div class="linebreak"></div>

    Our lab's computing resources include 32 RTX 3090/4090 GPUs and a cluster of 24 A6000 GPUs with a 100TB NAS. See <a href="https://www.linkedin.com/feed/update/urn:li:activity:7099940517966200832/">this post</a>.
    And we have multiple mobile platforms with robot arms and dex hands:

    <div class="linebreak"></div>

    <!-- <img src="resources/robot1.png" style="height:300px;margin:20px 30px 20px 0"></img> -->
    <img src="resources/teleOp_dog_arm_tennis_stand_oncourt_noppl.gif" style="height:300px;margin:20px 30px 20px 0"></img>
    <img src="resources/robotdoghand_open_door.gif" style="height:300px;margin:20px 30px 20px 0"></img>
    <img src="resources/dex_handaliengo_grab_things.gif" style="height:300px;margin:20px 30px 20px 0"></img>
    <img src="resources/robot3.png" style="height:300px;margin:20px 30px 20px 0"></img>

    <br/>

    Check out our lab's <span style="font-weight: bold;"><a href="./index.html#publications">cool publications and demos</a></span>.

    <br/>

    Our lab has over 10K followers on social media：
    [<a href="https://www.zhihu.com/people/Iris0329">Rong's 知乎</a>]
    [<a href="https://www.zhihu.com/people/30-76-66-6">Yujin's 知乎</a>]
    [<a href="https://www.zhihu.com/people/junwei-liang-50">Junwei's 知乎</a>]
    [<a href="https://www.xiaohongshu.com/user/profile/62c3a783000000001b02b099">Junwei's 小红书</a>]
    [<a href="https://www.linkedin.com/in/junweiliang/">Junwei's LinkedIn</a>]

  </div>



  <div class="title">
    <a class="title_link" id="media" href="#media">Media Coverage</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="label label-success">07/2024</span> 在世界人工智能大会发表“面向通用服务的具身智能”演讲
        [<a href="https://online2024.worldaic.com.cn/forumdetail?uuid=6e9fca0d377844e085fe7211f300ca19">上海WAIC</a>]
        [<a href="https://mp.weixin.qq.com/s/qZsHR-3adiDuku6ynwmcFg">联汇科技</a>]
      </li>
      <li>
        <span class="label label-success">06/2024</span> 在香港科技大学（广州）第二届INNOTECH展示机器狗和灵巧手Demo
        [<a href="https://mp.weixin.qq.com/s/iBIPX0iIQXUF4z_6IG2lRA">香港科技大学（广州）第二届INNOTECH创科嘉年华再创辉煌</a>]
      </li>
      <li>
        <span class="label label-success">06/2024</span> 三沙卫视采访
        [<a href="https://www.ssws.tv/p/52195.html">打造大湾区科创品牌盛会 香港科技大学（广州）创科嘉年华举办</a>]
      </li>
      <li>
        <span class="label label-success">05/2024</span> WAIC · 云帆奖五周年：AI 青年，执掌未来十年的钥匙
        [<a href="https://mp.weixin.qq.com/s/PIDdgctm-AFux6UjPh3Vsw">全球高校人工智能学术联盟</a>]
      </li>
      <li>
        <span class="label label-success">04/2024</span> 大模型“越狱” 如何监管开发者
        [应邀 <a href="https://gzdaily.dayoo.com/h5/html5/2024-04/29/content_878_856859.htm">广州日报</a> 采访]
        [HKUST(GZ) under the spotlight (April-May 2024, Issue 1) <a href="https://huacheng.gz-cmc.com/pages/2024/04/26/42405655b18a44a192ff833c0e7538f8.html">(Link)</a>]
      </li>
      <li>
        <span class="label label-success">04/2024</span> 他的代码在NASA上天，在港科广落地
        [by <a href="https://mp.weixin.qq.com/s/pUpWrnDtIm5gK_ZWOWhxHg">HKUST(GZ)</a>]
        [<a href="https://mp.weixin.qq.com/s/WJxtyM2FRzOABdz_N35a6A">INFO Hub</a>]
        [<a href="https://mp.weixin.qq.com/s/5smO2jbZV7_OOXIecxWNPw">AI Thrust</a>]
      </li>
      <li>
        <span class="label label-success">02/2024</span> 华人CMU校友回国创业，自研具身智能机器人，致力于开放场景的商业化落地
        [by <a href="https://mp.weixin.qq.com/s/f8NfHPlF208xEVRmtBocZg">DeepTech深科技</a>]
      </li>
      <li>
        <span class="label label-success">12/2023</span> 雅可比机器人获得2023新一代人工智能（深圳）创业大赛三等奖
        [by <a href="https://m.163.com/tech/article/IL7I2EOJ00097U7R.html">163 news</a>, <a href="https://mp.weixin.qq.com/s/x9BYlCXb7BTQvJcZMYFnzQ">雅可比机器人</a>]
      </li>
      <li>
        <span class="label label-success">10/2023</span> Patch才是时序预测的王道？
        [by <a href="https://mp.weixin.qq.com/s/JBO7_xyMwsATO_zi4LWbzQ">圆圆的算法笔记</a>, <a href="https://mp.weixin.qq.com/s/ElRBgAWTveItXJSoZe2DLQ">kaggle竞赛宝典</a>]
        [<a href="https://browse.arxiv.org/pdf/2310.00655v1.pdf">Paper</a>]
      </li>
      <li>
        <span class="label label-success">10/2023</span> TFNet：利用时间线索实现快速且精确的激光雷达语义分割
        [by <a href="https://mp.weixin.qq.com/s/V5uuTEwC8gFepIdXzFYAxA">自动驾驶专栏</a>]
        [<a href="https://arxiv.org/pdf/2309.07849.pdf">Paper</a>]
      </li>
      <li>
        <span class="label label-success">06/2023</span> Honorable Mention at the International Robot Manipulation Competition, Robothon (2023机器人马拉松挑战赛)
        [by <a href="https://mp.weixin.qq.com/s/V5uuTEwC8gFepIdXzFYAxA">香港科技大学（广州）公众号</a>]
        [<a href="https://automatica-munich.com/en/munich-i/robothon/teams/#team_submissions">Robothon 2023</a>]
      </li>
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="news" href="#news">News</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="label label-info">07/2024</span>
        Two papers accepted at ECCV 2024.
      </li>
      <li>
        <span class="label label-info">05/2024</span>
        1 paper accepted at <a href="https://teleema.github.io/projects/SADE/sade.html">NAACL 2024</a>.
        1 paper accepted at ACL 2024. Main conference.
      </li>
      <li>
        <span class="label label-info">02/2024</span>
        Serve as a panelist at the VALSE Embodied AI webinar.
        [<a href="https://mp.weixin.qq.com/s/788DpNUbjklH-unhznkewg?poc_token=HJlaAmaj-hqsfo_KezM6IrqyY7MsZZ5wYfVpxEyz">VALSE</a>]
        [<a href="https://live.bilibili.com/22300737">bilibili</a>]

      </li>
      <li>
        <span class="label label-info">02/2024</span> Co-organizing the <a href="https://sites.google.com/view/ieeecvf-cvpr2024-precognition">The 6th workshop on Precognition: Seeing through the Future</a> @CVPR 2024.
        [<a href="https://www.linkedin.com/posts/junweiliang_cvpr2024-computervision-visionforecasting-activity-7163724230201733120-ERWa/">Call For Papers</a>] [<a href="https://zhuanlan.zhihu.com/p/682320005">知乎</a>] [<a href="https://www.xiaohongshu.com/explore/65cd76e400000000070055a8">小红书</a>]
      </li>
      <li>
        <span class="label label-info">12/2023</span> Keynote speech at the CEII2023 Workshop
        [<a href="https://mp.weixin.qq.com/s/FRZ_r2BrvCz2Y7RreJdLgA">Schedule</a>]
      </li>
      <li>
        <span class="label label-info">10/2023</span> Co-organizing the Open-world Visual Perception Workshop (“开放世界下的视觉感知和增强”主题论坛) @PRCV 2023
        [<a href="https://mp.weixin.qq.com/s/ib9aKBhQhoaAFqZB93F3wQ">Schedule</a>]
      </li>
      <li>
        <!--<a href="https://www.linkedin.com/posts/khoa-luu-90900215_cvpr2023-activity-7009293998955655168-M4lu?utm_source=share&utm_medium=member_desktop">Precognition workshop</a>-->
        <span class="label label-info">01/2023</span> Co-organizing the <a href="https://sites.google.com/view/ieeecvf-cvpr2023-precognition">The 5th workshop on Precognition: Seeing through the Future</a> @CVPR 2023.
        [<a href="https://www.linkedin.com/posts/junweiliang_cvpr2023-workshop-computervision-activity-7030466054787121152-NacF?utm_source=share&utm_medium=member_desktop">Call For Papers</a>] [<a href="https://zhuanlan.zhihu.com/p/603134088">知乎</a>]
      </li>
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="members" href="#members">Lab Members</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <a href="https://teleema.github.io/">Teli Ma</a>
        [<a href="https://github.com/TeleeMa">Github</a>]
        [<a href="https://scholar.google.com/citations?user=arny77IAAAAJ">Google Scholar</a>]
        [<a href="https://www.zhihu.com/people/yin-he-xi-xiao-wang-zi">知乎</a>]
      </li>

      <li>
        <a href="https://jiaming-zhou.github.io/">Jiaming Zhou</a>
        [<a href="https://github.com/jiaming-zhou/">Github</a>]
        [<a href="https://scholar.google.com/citations?user=b3y40w8AAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="https://zifanw.notion.site/">Zifan Wang</a>
        [<a href="https://github.com/aCodeDog">Github</a>]
        [<a href="https://scholar.google.com/citations?user=GaJXZ-UAAAAJ&hl=zh-CN">Google Scholar</a>]
      </li>

      <li>
        <a href="http://www.davidqiu.com/">Dicong Qiu</a>
        [<a href="https://github.com/davidqiu1993">Github</a>]
        [<a href="https://scholar.google.com/citations?user=ZFmSow8AAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="https://openreview.net/profile?id=~Ronghe_Qiu2">Ronghe Qiu</a>
        [<a href="https://github.com/ConnerQiu">Github</a>]
        [<a href="https://scholar.google.com/citations?hl=en&user=QOwOHZMAAAAJ">Google Scholar</a>]
      </li>
      <li>
        <a href="https://zeying-gong.github.io/">Zeying Gong</a>
        [<a href="https://github.com/Zeying-Gong">Github</a>]
        [<a href="https://scholar.google.com/citations?user=ze2Wh9EAAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="https://rongli.tech/">Rong Li</a>
        [<a href="https://www.zhihu.com/people/Iris0329">知乎</a>]
        [<a href="https://github.com/iris0329">Github</a>]
        [<a href="https://scholar.google.com/citations?user=M68wBgkAAAAJ">Google Scholar</a>]
      </li>
      <li>
        <a href="https://hutslib.github.io/">Tianshuai Hu</a>
        [<a href="https://github.com/hutslib">Github</a>]
        [<a href="https://scholar.google.com.hk/citations?user=RJ7NR54AAAAJ&hl=zh-CN">Google Scholar</a>]
      </li>
      <li>
        <a href="https://www.jiayi-liu.cn/">Jiayi Liu</a>
        [<a href="https://github.com/juceyj">Github</a>]
      </li>

      <li>
        <a href="https://zgzxy001.github.io/">Xiaoyu Zhu</a> (Ph.D. student @CMU, co-advising)
        [<a href="https://github.com/zgzxy001">Github</a>]
        [<a href="https://twitter.com/XiaoyuZhu10">Twitter</a>]
        [<a href="https://scholar.google.com/citations?user=T4Dc5zEAAAAJ&hl=en">Google Scholar</a>]
      </li>
      <li>
        <a href="https://jhuiye.com/">Jinhui Ye</a>
        [<a href="https://github.com/JinhuiYE">Github</a>]
        [<a href="https://scholar.google.com/citations?user=RpXhJu0AAAAJ">Google Scholar</a>]
      </li>
      <li>
        <a href="https://scholar.google.com/citations?user=bRq33Q8AAAAJ">Sheng Wang</a>
        [<a href="https://scholar.google.com/citations?user=bRq33Q8AAAAJ">Google Scholar</a>]
        [<a href="https://www.researchgate.net/profile/Wang-Sheng-41">Research Gate</a>]
      </li>

      <hr/>

      <li>
        <a href="http://www.linkedin.com/in/jian-chen-a7501517b">Jian Chen</a>
        (Now with HSBC and pursuing a PhD)
        [<a href="https://scholar.google.com/citations?user=fmPehrUAAAAJ&hl=en&oi=sra">Google Scholar</a>]
        [<a href="https://github.com/AlexJJJChen">Github</a>]
      </li>
      <li>
        <a href="https://github.com/yyyujintang">Yujin Tang</a>
        (Now with <a href="https://scholar.google.com/citations?user=p9-ohHsAAAAJ&hl=en">Ming-Hsuan Yang</a>)
        [<a href="https://www.zhihu.com/people/30-76-66-6">知乎</a>]
        [<a href="https://github.com/yyyujintang">Github</a>]
      </li>
      <li>
        <a href="https://github.com/XinyuSun">Xinyu Sun</a>
        (Now at DJI)
        [<a href="https://github.com/XinyuSun">Github</a>]
        [<a href="https://scholar.google.com/citations?user=ALq8sMgAAAAJ">Google Scholar</a>]
      </li>

    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="publications" href="#publications">Publications</a>
  </div>

  <div class="content publications">
    * indicates corresponding authors.
    <ol>
      Preprint.
      <li>
        <div class="imgblock"><img src="camera_ready/jiaming01.png"></img></div>
        <span class="title">Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation
        </span>
        <div class="info text-success italic">
          Jiaming Zhou, Teli Ma, Kun-Yu Lin, Ronghe Qiu, Zifan Wang, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">ArXiv 2024</span>
          <!--<span class="text-error">(Spotlight paper)</span>-->
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/pdf/2406.14235">Paper</a>]
          [<a href="https://jiaming-zhou.github.io/projects/HumanRobotAlign/">Project page</a>]
        </div>
        <div style="clear:both"></div>
      </li>


      <li>
        <div class="imgblock"><img src="camera_ready/teli01.gif"></img></div>
        <span class="title">Contrastive Imitation Learning for Language-guided Multi-Task Robotic Manipulation
        </span>
        <div class="info text-success italic">
         Teli Ma, Jiaming Zhou, Zifan Wang, Ronghe Qiu, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">ArXiv 2024</span>
          <!--<span class="text-error">(Spotlight paper)</span>-->
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/pdf/2406.09738">Paper</a>]
          [<a href="https://teleema.github.io/projects/Sigma_Agent/">Project page</a>]
          [<a href="https://youtu.be/t6xLTnMFTB8">Video</a>]
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/dicong01.gif"></img></div>
        <span class="title">Open-vocabulary Mobile Manipulation in Unseen Dynamic Environments with 3D Semantic Maps
        </span>
        <div class="info text-success italic">
         Dicong Qiu, Wenzong Ma, Zhenfu Pan, Hui Xiong, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">ArXiv 2024</span>
          <!--<span class="text-error">(Spotlight paper)</span>-->
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/pdf/2406.18115">Paper</a>]
          [<a href="https://davidqiu1993.github.io/3dsmaps/">Project page</a>]
          [<a href="https://youtu.be/xE6M6WKw-0k">Video</a>]
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/jian01.png"></img></div>
        <span class="title">Vision-Language Models Meet Meteorology: Developing Models for Extreme Weather Events Detection with Heatmaps
        </span>
        <div class="info text-success italic">
          Jian Chen, Peilin Zhou, Yining Hua, Dading Chong, Meng Cao, Yaowei Li, Zixuan Yuan, Bing Zhu, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">ArXiv 2024</span>
          <!--<span class="text-error">(Spotlight paper)</span>-->
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/pdf/2406.09838">Paper</a>]
          [<a href="https://github.com/AlexJJJChen/Climate-Zoo">Code/Data</a>]
        </div>
        <div style="clear:both"></div>
      </li>

      <hr/>

      <li>
        <div class="imgblock"><img src="camera_ready/xinyu01.png"></img></div>
        <span class="title">Prioritized Semantic Learning for Zero-shot Instance Navigation
        </span>
        <div class="info text-success italic">
         Xinyu Sun, Lizhao Liu, Hongyan Zhi, Ronghe Qiu, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">ECCV 2024</span>
          <!--<span class="text-error">(Spotlight paper)</span>-->
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/pdf/2403.11650">Paper</a>]
          [<a href="https://github.com/XinyuSun/PSL-InstanceNav">Dataset/Code/Model</a>]
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/naacl24.png"></img></div>
        <span class="title">An Examination of the Compositionality of Large Generative Vision-Language Models
        </span>
        <div class="info text-success italic">
          Teli Ma, Rong Li, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">NAACL 2024</span>
          <!--<span class="text-error">(Spotlight paper)</span>-->
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2308.10509">Paper</a>]
          [<a href="https://teleema.github.io/projects/SADE/sade.html">Project Page</a>]
          [<a href="https://github.com/TeleeMa/SADE">Dataset</a>]
          [<a href="https://zhuanlan.zhihu.com/p/703924239">知乎</a>]
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/acl24.png"></img></div>
        <span class="title">FinTextQA: A Dataset for Long-form Financial Question Answering
        </span>
        <div class="info text-success italic">
          Jian Chen, Peilin Zhou, Yining Hua, Yingxin Loh, Kehui Chen, Ziyuan Li, Bing Zhu*, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">ACL 2024</span>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2405.09980">Paper</a>]
          [<a href="https://github.com/AlexJJJChen/FinTextQA">Code/Model</a>]
        </div>
        <div style="clear:both"></div>
      </li>


      <li>
        <div class="imgblock"><img src="camera_ready/cvpr24precog.png"></img></div>
        <span class="title">VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate Spatiotemporal Forecasting
        </span>
        <div class="info text-success italic">
          Yujin Tang, Peijie Dong, Zhenheng Tang, Xiaowen Chu, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">CVPR 2024 Precognition Workshop</span>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2403.16536">Paper</a>]
          [<a href="https://github.com/yyyujintang/VMRNN-PyTorch">Project Page/Code/Model</a>]
          <iframe src="https://ghbtns.com/github-btn.html?user=yyyujintang&repo=VMRNN-PyTorch&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/ijcai24.png"></img></div>
        <span class="title">PatchMixer: A Patch-Mixing Architecture for Long-Term Time Series Forecasting
        </span>
        <div class="info text-success italic">
          Zeying Gong, Yujin Tang, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">IJCAI 2024 Workshop: DATA SCIENCE MEETS OPTIMISATION</span>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2310.00655">Paper</a>]
          [<a href="https://github.com/Zeying-Gong/PatchMixer">Project Page/Code/Model</a>]
          <iframe src="https://ghbtns.com/github-btn.html?user=Zeying-Gong&repo=PatchMixer&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/postrainbench.png"></img></div>
        <span class="title">PostRainBench: A comprehensive benchmark and a new model for precipitation forecasting
        </span>
        <div class="info text-success italic">
          Yujin Tang, Jiaming Zhou, Xiang Pan, Zeying Gong, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">ICLR 2024 Workshop: Tackling Climate Change with Machine Learning</span>
          <span class="text-error">(Spotlight paper)</span>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2310.02676">Paper</a>]
          [<a href="https://github.com/yyyujintang/PostRainBench">Project Page/Code/Model</a>]
          <iframe src="https://ghbtns.com/github-btn.html?user=yyyujintang&repo=PostRainBench&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/cvpr24wad.png"></img></div>
        <span class="title">TFNet: Exploiting Temporal Cues for Fast and Accurate LiDAR Semantic Segmentation
        </span>
        <div class="info text-success italic">
          Rong Li, ShiJie Li, Xieyuanli Chen, Teli Ma, Wang Hao, Juergen Gall, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">CVPR 2024 Workshop on Autonomous Driving</span>
        </div>
        <div class="stuff">
          [<a href="https://openaccess.thecvf.com/content/CVPR2024W/WAD/papers/Li_TFNet_Exploiting_Temporal_Cues_for_Fast_and_Accurate_LiDAR_Semantic_CVPRW_2024_paper.pdf">Paper</a>]
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/cvpr23.png"></img></div>
        <span class="title">STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition
        </span>
        <div class="info text-success italic">
          Xiaoyu Zhu, Po-Yao Huang, <span style="font-weight:bold">Junwei Liang</span>, Celso M de Melo, Alexander G Hauptmann
        <div class="info"><span class="label label-info">CVPR 2023</span>
        </div>
        <div class="stuff">
          [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_STMT_A_Spatial-Temporal_Mesh_Transformer_for_MoCap-Based_Action_Recognition_CVPR_2023_paper.pdf">Paper</a>]
          [<a href="https://github.com/zgzxy001/STMT">Project Page/Code/Model</a>]
          <iframe src="https://ghbtns.com/github-btn.html?user=zgzxy001&repo=STMT&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div style="clear:both"></div>
      </li>

      <!-- Publication from before -->

      <li>
        <div class="imgblock"><img src="camera_ready/cross_action.jpg"></img></div>
        <span class="title">Multi-dataset Training of Transformers for Robust Action Recognition
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Enwei Zhang, Jun Zhang, Chunhua Shen</div>
        <div class="info"><span class="label label-info">NeurIPS 2022</span>
          <span class="text-error">(<a href="https://nips.cc/virtual/2022/spotlight/65262">Spotlight paper</a>, 3.7% acceptance rate, 384/10411)</span>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2209.12362">Paper</a>]
          [<a href="https://github.com/JunweiLiang/MultiTrain">Project Page/Code/Model</a>]
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=MultiTrain&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/multiverse.gif"></img></div>
        <span class="title">The Garden of Forking Paths: Towards Multi-Future Trajectory Prediction
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Lu Jiang, Kevin Murphy, Ting Yu, Alexander Hauptmann</div>
        <div class="info"><span class="label label-info">CVPR 2020</span> &nbsp;
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=Multiverse&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/1912.06445" target="_blank">[Paper]</a>
          <a class="" href="https://precognition.team/next/multiverse/resources/cvpr2020.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://youtu.be/RW45YQHxIhk" target="_blank">[Demo Video]</a>
          <a class="" href="https://precognition.team/next/multiverse" target="_blank">[Project Page/Code/Model]</a>
          <a href="https://medium.com/@junweil/cvpr20-the-garden-of-forking-paths-towards-multi-future-trajectory-prediction-df23221dc9f8">[blog]</a>
          <a href="https://zhuanlan.zhihu.com/p/148343447">[知乎]</a>
          <a href="https://research.google/pubs/pub49224/">[Google Research]</a>
          <a href="https://mp.weixin.qq.com/s/s6bk5psLwpGpO1VwtQqo_g">[读芯术学术报告]</a>
          <a href="https://sites.google.com/di.uniroma1.it/patcast/program?authuser=0">[Invited presentation at ICPR'20 pattern forecasting workshop]</a>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <!--<div class="imgblock"><img src="camera_ready/peekfuture.png"></img></div>-->
        <div class="imgblock" style="height:280px"><img src="camera_ready/next.gif"></img></div>
        <span class="title">Peeking into the Future: Predicting Future Person Activities and Locations in Videos
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Lu Jiang, Juan Carlos Niebles, Alexander Hauptmann, Li Fei-Fei</div>
        <div class="info"><span class="label label-info">CVPR 2019</span> <span class="text-error">(Translated and reported by multiple Chinese media (<a href="https://weixin.sogou.com/weixin?type=1&s_from=input&query=%E9%87%8F%E5%AD%90%E4%BD%8D" target="_blank">量子位</a> & <a href="https://weixin.sogou.com/weixin?type=1&s_from=input&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83" target="_blank">机器之心</a>, 02/13/2019), with 30k+ views in a week.)</span> </div>
        <div class="info"><span class="text-error">#1 Tensorflow-based code on <a href="https://paperswithcode.com/task/trajectory-prediction">PaperWithCode</a> in Trajectory Prediction task. </span> <iframe src="https://ghbtns.com/github-btn.html?user=google&repo=next-prediction&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/1902.03748" target="_blank">[Paper]</a>
          <a class="" href="camera_ready/future19.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://www.youtube.com/watch?v=NyrGxGoS01U" target="_blank">[Demo Video]</a>
          <a class="" href="https://precognition.team/next" target="_blank">[Project Page/Code/Model]</a>
          <a href="https://research.google/pubs/pub47873/">[Google Research]</a>
        </div>
        <div style="clear:both"></div>
      </li>



    </ol>
  </div>



</div>


<!--
	a Junwei Liang's production
	contact: junweiliang1114@gmail.com
-->
</body>
</html>
