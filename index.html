<html>
<head>
	<meta  http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=1" />
	<link rel="stylesheet" type="text/css" href="utils/bootstrap.min.css"/>
  <script language="javascript" src="utils/jquery.min.js"></script>
	<script language="javascript" src="utils/bootstrap.min.js"></script>
  <!-- -->
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css"/>
	<link rel="stylesheet" type="text/css" href="utils/cssReset.css"/>
  <link rel="stylesheet" type="text/css" href="utils/css_layout.css"/>
	<title>Precognition Lab at HKUST (Guangzhou)</title>
  <!-- æœç´¢å¼•æ“ä¼˜åŒ–stuff -->
	<meta name="description"
    content="Academic website for Prof. Junwei Liang's lab, the Precognition Lab. Dr. Junwei Liang is currently a tenure-track Assistant Professor at The Hong Kong University of Science and Technology (Guangzhou). He is also an affiliate assistant professor at HKUST computer science & engineering department. He was a senior researcher at Tencent Youtu Lab working on cutting-edge computer vision research and applications. Prior to that, he received his Ph.D. degree from Carnegie Mellon University, working with Prof. Alexander Hauptmann. He is the recipient of Baidu Scholarship and Yahoo Fellowship, and awarded Rising Star Award at the World AI Conference in 2020. He is the winner of several public safety video analysis competitions, including ASAPS and TRECVID ActEV. His work has helped and been reported by major news agencies like the Washington Post and New York Times. His research interests include human trajectory forecasting, action recognition, and large-scale computer vision and video analytics in general. His mission: develop AI technologies for social good.">
	<meta name="keywords" content="Junwei Liang,CMU,HKUST,HKUST-GZ,Professor,computer vision,EmbodiedAI,PhD,æ¢ä¿Šå«,Carnegie Mellon University,The Hong Kong University of Science and Technology">

	<!-- Global site tag (gtag.js) - Google Analytics/ no use after 06/2024 -->
  <!--
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156016426-1"></script>
  <script>
  	// for Google Analytics, for free!
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-156016426-1');

  </script>
-->

</head>
<body>
<div id="sidebar">
  <!--<img class='me' src="resources/me.jpeg"></img>-->
  <img class='me' src="resources/precognition_lab.png"></img>
  <br/>
  <div class="info">
    <h2 class="name">Lab Director:</h2>
    <h2 class="name">Prof. Junwei Liang</h2>
    <h2 class="name_chinese">æ¢ä¿Šå«</h2>
    <h2 class="email">HKUST (Guangzhou) / HKUST</h2>
    <h2 class="email">Office: E4-304</h2>
    <h2 class="link">
      <a style="font-size: 14px; color: yellow; font-weight: bold;" href="https://junweiliang.me">Personal Page</a>
    </h2>
  <!--
    <h2 class="link">
      <a href="https://www.semanticscholar.org/author/Junwei-Liang/1915796">[Semantic Scholar]</a>
      <a href="https://www.researchgate.net/profile/Junwei_Liang3">[Research Gate]</a>
    </h2>
    <h2 class="link">
      <a href="https://github.com/JunweiLiang">[Github]</a>
      <a href="https://www.linkedin.com/in/junweiliang/">[LinkedIn]</a>
      <a href="https://www.youtube.com/channel/UC-z7ZWp8Rbu2xhxnbAL_bRQ">[Youtube]</a>
    </h2>
    <h2 class="link">
      <a href="https://www.zhihu.com/people/junwei-liang-50">[çŸ¥ä¹]</a>
      <a href="https://www.xiaohongshu.com/user/profile/62c3a783000000001b02b099">[å°çº¢ä¹¦]</a>
      <a href="https://twitter.com/JunweilLiang">[Twitter]</a>
    </h2>
  -->

  </div>
  <div id="navigation">
    <a class="nav_item" href="./index.html">
      <i class="icon icon-home icon-white"></i> &nbsp; About
    </a>
    <a class="nav_item" href="./index.html#publications">
      <i class="icon icon-th-large icon-white icon-list"></i> &nbsp; Publications
    </a>
    <a class="nav_item" href="./projects.html#projects">
      <i class="icon icon-th-large icon-white"></i> &nbsp; Projects
    </a>
    <a class="nav_item" href="./index.html#members">
      <i class="icon icon-white icon-user"></i> &nbsp; Lab Members
    </a>
    <a class="nav_item" href="./internal/index.html">
      <i class="icon icon-book icon-white"></i> &nbsp; Internal Docs
    </a>

  </div>
</div>

<div id="main">
  <div class="title" style="padding:40px 0">
    <a class="title_link" id="bio" href="#bio">Precognition Lab @ HKUST (Guangzhou)</a>
    <!--<img src="https://vbr.wocr.tk/badge?page_id=JunweiLiang.precognition&right_color=green" alt=""/> -->
    <img src="https://vbr.wocr.tk/badge?page_id=JunweiLiang.JunweiLiang&right_color=green" alt=""/>
    <img id="logo" src="resources/hkustgz-logo.jpg"></img>
  </div>


  <div class="content">
    Our research lab, the Precognition Lab (æ™ºèƒ½æ„ŸçŸ¥ä¸é¢„æµ‹å®éªŒå®¤), is interested in building human-level <span style="font-weight: bold;">Embodied AI</span> systems that can effectively perceive, reason and interact with the real world for the good of humans.
    Here is an up-to-date <a href="./projects.html#roadmap">research roadmap</a>.

    <div class="linebreak"></div>

    Our lab's computing resources include <b>36 RTX 3090/4090/L40 GPUs and a cluster of 24 A6000 GPUs</b> with a 100TB NAS. See <a href="https://www.linkedin.com/feed/update/urn:li:activity:7099940517966200832/">this post</a>.
    And we have multiple mobile platforms with robot arms and dex hands:

    <div class="linebreak"></div>

    <!-- <img src="resources/robot1.png" style="height:300px;margin:20px 30px 0 0"></img> -->
    <img src="resources/g1+go2w.jpg" style="height:300px;margin:20px 10px 20px 0"></img>
    <img src="resources/lab_small.jpeg" style="height:300px;margin:20px 10px 20px 0"></img> <br/>
    <img src="resources/corl_omniperception.gif" style="height:300px;margin:20px 10px 20px 0"></img>
    <img src="resources/aliengo_tennis.gif" style="height:300px;margin:20px 10px 20px 0"></img>
    <img src="resources/corl_glover++.gif" style="height:300px;margin:20px 10px 20px 0"></img>
    <!--<img src="resources/robotdoghand_open_door.gif" style="height:300px;margin:20px 10px 20px 0"></img>-->
    <!--<img src="resources/dex_handaliengo_grab_things.gif" style="height:300px;margin:20px 10px 20px 0"></img>-->
    <img src="resources/go2_arm.png" style="height:300px;margin:20px 10px 20px 0"></img>
    <img src="resources/robot3.png" style="height:300px;margin:20px 10px 20px 0"></img>

    <br/>

    Check out our lab's <span style="font-weight: bold;"><a href="./index.html#publications">cool publications and demos</a></span>.
    <!--
    <br/>

    Our lab has over 10K followers on social mediaï¼š
    [<a href="https://www.zhihu.com/people/Iris0329">Rong's çŸ¥ä¹</a>]
    [<a href="https://www.zhihu.com/people/30-76-66-6">Yujin's çŸ¥ä¹</a>]
    [<a href="https://www.zhihu.com/people/junwei-liang-50">Junwei's çŸ¥ä¹</a>]
    [<a href="https://www.xiaohongshu.com/user/profile/62c3a783000000001b02b099">Junwei's å°çº¢ä¹¦</a>]
    [<a href="https://www.linkedin.com/in/junweiliang/">Junwei's LinkedIn</a>]
    -->

  </div>



  <div class="title">
    <a class="title_link" id="media" href="#media">Media Coverage</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="label label-success">å­¦æœ¯åª’ä½“æŠ¥é“ Academic Media Coverage</span> <span class="label label-info">å…¶ä»–åª’ä½“æŠ¥é“ Social Media Coverage</span>
      </li>

      <li>
        <span class="label label-success">02/2026</span>
        å…¥é€‰â€œä¸­å›½ç”µå­å­¦ä¼š-è…¾è®¯Robotics XçŠ€ç‰›é¸Ÿä¸“é¡¹ç ”ç©¶è®¡åˆ’ï¼ˆ2025ï¼‰â€
        [<a href="https://mp.weixin.qq.com/s/VsW95Elk_DOihoCBt1A5mw?scene=1">è…¾è®¯é«˜æ ¡åˆä½œ</a>]
      </li>
      <li>
        <span class="label label-success">01/2026</span>
        æ´»åŠ¨å›é¡¾ | æ¸…åå¤§å­¦ç”µå­å·¥ç¨‹ç³»å¸ˆç”Ÿä¸ä¸œèå¸‚æ¾å±±æ¹–æ¸…æ¾œå±±å­¦æ ¡å¸ˆç”Ÿåˆ°è®¿æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰
        [<a href="https://mp.weixin.qq.com/s/2OWxvnzyfTd36S7OiJIDZQ?scene=1">å®éªŒå®¤æ¥å¾…æ¸…åå¸ˆç”Ÿ</a>]
      </li>
      <li>
        <span class="label label-success">01/2026</span> ASCENT is accepted by IEEE Robotics and Automation Letters (RA-L)ï¼ˆæœºå™¨äººæ–¹å‘é¡¶åˆŠï¼‰!
        [<a href="https://zeying-gong.github.io/projects/ascent/">Project Page</a>]
        [<a href="https://arxiv.org/abs/2505.23019">Paper</a>]
        [<a href="https://github.com/Zeying-Gong/ascent">Code and model</a>]
        [<a href="https://mp.weixin.qq.com/s/kg4c9H_tS-I4xoJaOW-dXg">RAL-2026 | æ¸¯ç§‘å¹¿å¤šæ¥¼å±‚æœºå™¨äººå¯¼èˆªæ–°çªç ´ï¼ASCENT: å®ç°æ¥¼å±‚æ„ŸçŸ¥çš„é›¶æ ·æœ¬ç‰©ä½“ç›®æ ‡å¯¼èˆª</a>]
      </li>
      <li>
        <span class="label label-success">12/2025</span> Our Self-driving VLA Survey is released and reported by others!
        [<a href="https://mp.weixin.qq.com/s/ZZnYSzwJTVDwb-QQpv7xgw">ã€VLAç»¼è¿°ã€‘æ¸¯ç§‘æ²ˆåŠ­åŠ¼æ¢ä¿Šå«NTUåˆ˜å­çº¬ç­‰å›¢é˜Ÿè‡ªåŠ¨é©¾é©¶VLAé‡ç£…ç»¼è¿°ï¼</a>]
        [<a href="https://mp.weixin.qq.com/s/5bkaJCY4NCgsh9oe33NmmQ">æ·±è“AIæŠ¥é“</a>]
        [<a href="https://arxiv.org/abs/2512.16760">Paper</a>]
      </li>
      <li>
        <span class="label label-info">10/2025</span> 21ä¸–çºªè´¢ç»ä¸“è®¿
        [<a href="https://m.21jingji.com/article/20251208/herald/7d0d4108c457bb3abf676a1734beb3d6.html?t=772148">â€œæ¢ä¿Šå«ï¼šç»™å¤§æ¨¡å‹å®‰è£…â€œä¸€åŒæ‰‹â€è¦ç»å†å‡ æ­¥ï¼Ÿâ€</a>]
        [<a href="https://www.bilibili.com/video/BV1qV2SBSEoi/">Bç«™</a>]
      </li>
      <li>
        <span class="label label-success">09/2025</span> GLOVERåœ¨CoRL GenPriors Workshopè·å¾—
        <span style="font-weight:bold;color:red">æœ€ä½³è®ºæ–‡å¥–(Best Paper Award)ğŸ¥‡</span>
        [<a href="https://www.xiaohongshu.com/explore/68d7a6a00000000012030773?app_platform=ios&app_version=9.1.2&share_from_user_hidden=true&xsec_source=app_share&type=normal&xsec_token=CBiPKCZngxg470muJsud7g2j4JtmYQtJ35q8GndUH3grk=&author_share=1&xhsshare=WeixinSession&shareRedId=ODhFM0Q8PDw2NzUyOTgwNjdFOTlGNT9C&apptime=1758963375&share_id=c61d61af7751472aa32764b87b05d9db">å°çº¢ä¹¦æŠ¥é“</a>]
        [<a href="https://x.com/JunweiLiangCMU/status/1972109244108623987">TwitteræŠ¥é“</a>]
        [<a href="https://teleema.github.io/projects/GLOVER++/">é¡¹ç›®ç½‘ç«™</a>]
      </li>
      <li>
        <span class="label label-info">09/2025</span> å¹¿ä¸œçœå¼€å­¦ç¬¬ä¸€è¯¾ï¼šçœæ•™è‚²å…é‚€è¯·é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰äººå·¥æ™ºèƒ½å­¦åŸŸåŠ©ç†æ•™æˆæ¢ä¿Šå«åšäººå·¥æ™ºèƒ½ä¸æœºå™¨äººäº§ä¸šç›¸å…³æŠ€æœ¯çŸ¥è¯†ç§‘æ™®å®£è®²
        [<a href="https://gdii.gd.gov.cn/tpxw987/content/post_4775401.html">å¹¿ä¸œçœå·¥ä¿¡å…</a>]

        [<a href="https://www.youtube.com/watch?v=VuL3wP8fuRQ">å¹¿ä¸œç»æµç§‘æ•™é¢‘é“ï¼šå¹¿ä¸œå¼€å­¦ç¬¬ä¸€è¯¾</a>]
        [<a href="https://cj.sina.com.cn/articles/view/2131593523/7f0d893302001lhcg?froms=ggmp">ç¾ŠåŸæ™šæŠ¥</a>]
      </li>
      <li>
        <span class="label label-success">08/2025</span> ç¬¬ä¸‰æ–¹åª’ä½“æŠ¥é“OmniPerceptionç ”ç©¶å·¥ä½œ
        <span style="font-weight:bold;color:red">(5%å½•å–ç‡ï¼Œå¤§æ¹¾åŒºå”¯ä¸€çš„CoRL Oral paper)</span>
        [<a href="https://mp.weixin.qq.com/s/7gyV4iSF1G4sSyuoDRRMDg">ä»…å æŠ•ç¨¿5% | æ¨èå…¥é€‰ CoRL 2025 Oral çš„17 ç¯‡ç¡¬æ ¸æˆæœï¼Œçœ‹æœºå™¨äººå­¦ä¹ æ–°è¶‹åŠ¿ï¼</a>]
        [<a href="https://acodedog.github.io/OmniPerceptionPages/">é¡¹ç›®ç½‘ç«™</a>]
        [<a href="https://mp.weixin.qq.com/s/TM6Js3wAn1jai2oUyyo30Q?scene=1">æ·±è“å…·èº«æ™ºèƒ½è§£è¯»</a>]
        [<a href="https://mp.weixin.qq.com/s/r9JvBsPf9jBQqiRe2UXVgQ?scene=1">æ™ºçŒ©çŒ©Robotè§£è¯»</a>]
      </li>
      <li>
        <span class="label label-info">07/2025</span> å¹¿å·æ—¥æŠ¥ä¸“è®¿
        [<a href="https://huacheng.gz-cmc.com/pages/2025/07/21/SF14148463618aaebf888241f4a893e0.html">â€œèƒ½ä¸»åŠ¨ä¸ºè¡Œäººè®©è·¯çš„æœºå™¨äººï¼Œæ‰æ˜¯å¥½æœºå™¨äººâ€</a>]
        [<a href="https://mp.weixin.qq.com/s/Ym9UxqPLb6OSZxrtxCeHqw?scene=1">ã€Šå¹¿å·æ—¥æŠ¥ Â· ç§‘æŠ€å‘¨åˆŠã€‹æ­ç§˜æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰äººå½¢æœºå™¨äººâ€œå¤§è„‘â€</a>]
      </li>
      <li>
        <span class="label label-info">07/2025</span>
        Make a humanoid robot demo with manipulation for the university.
        æ”¯æŒæ¸¯ç§‘å¹¿2025çº§æœ¬ç§‘ç”Ÿå½•å–æ´»åŠ¨ï¼Œäººå½¢æœºå™¨äººé€’é€å½•å–é€šçŸ¥ä¹¦
        [<a href="https://mp.weixin.qq.com/s/jIJfb1G1MPbs-GTfKwNhCQ?scene=1">ç§‘æŠ€èµ‹èƒ½è¿æ–° æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰ä¸¾åŠæœ¬ç§‘æ–°ç”Ÿä»£è¡¨è§é¢æ´»åŠ¨</a>]
        [å¾®ä¿¡è§†é¢‘å·ï¼šé¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰- ç›´å‡»æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰ç¡¬æ ¸æœ¬ç§‘æ–°ç”Ÿä»£è¡¨è§é¢ä¼šç°åœºï¼]
        [<a href="https://www.xiaohongshu.com/explore/687c52b5000000000b02d825?source=webshare&xhsshare=pc_web&xsec_token=ABrVSHQdRMREeTYR1Q0zKAOpzcRqmEAnJF5Rf1SCtAEvk=&xsec_source=pc_share">Demo Video - å°çº¢ä¹¦</a>]
        [å¾®ä¿¡è§†é¢‘å·ï¼šç¾ŠåŸæ™šæŠ¥ - æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰æœ¬ç§‘æ–°ç”Ÿä»£è¡¨è§é¢ä¼šï¼šäººå½¢æœºå™¨äººé€è¾¾å½•å–é€šçŸ¥ä¹¦]
        [å¾®ä¿¡è§†é¢‘å·ï¼šå¹¿å·å¹¿æ’­ç”µè§†å° - ä½ çš„æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰å½•å–é€šçŸ¥ä¹¦ï¼Œäººå½¢æœºå™¨äººåœ¨å¼€ç€â€œç–¾è·‘ â€é€æ¥ï¼]
      </li>
      <li>
        <span class="label label-success">06/2025</span>
        We are organizing a Social Navigation Competition at IROS 2025.
        æˆ‘ä»¬æ­£åœ¨ä¸¾åŠIROS 2025æœºå™¨äººç¤¾äº¤å¯¼èˆªæ¯”èµ›ï¼Œæ¬¢è¿å‚èµ›ï¼åæœˆæ­å·è§ï¼
        [<a href="https://robosense2025.github.io/#organizers">website æ¯”èµ›ç½‘ç«™</a>]
        [<a href="https://x.com/JunweilLiang/status/1930881503636127887">Tweet</a>]
        [<a href="https://www.xiaohongshu.com/discovery/item/68423902000000002202801a?source=webshare&xhsshare=pc_web&xsec_token=ABSybfxl1fFiqIS3y8k-ka_SSRaJMuSZbvX3mBfjhWbp8=&xsec_source=pc_share">å°çº¢ä¹¦å®£ä¼ </a>]
        [<a href="https://zhuanlan.zhihu.com/p/1914310707429737712">çŸ¥ä¹</a>]
      </li>
      <li>
        <span class="label label-success">06/2025</span>
        åœ¨æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰INNOTECHæ´»åŠ¨ç°åœºæ¼”ç¤ºæœ€æ–°locomotionã€äººå½¢æœºå™¨äººç§‘ç ”æˆæœã€‚
        Presented Live Demos at INNOTECH 2025@HKUST-GZ.
        [<a href="https://mp.weixin.qq.com/s/7XNbaXuVAi2WI2HRrt6GWg?scene=1">å¹¿å·å—æ²™å‘å¸ƒ</a>]
        [<a href="https://mp.weixin.qq.com/s/LvXlZdRN2Se95bAwxElNrw?scene=1">é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰</a>]
        [<a href="https://www.stdaily.com/web/gdxw/2025-06/21/content_358337.html">ç§‘æŠ€æ—¥æŠ¥</a>]
        [<a href="https://www.scmp.com/presented/news/hong-kong/education/topics/hkustgz-blazes-trail-innovation-led-universities/article/3329411/hkustgz-blazes-trail-innovation-led-universities">South China Morning Post</a>]
      </li>
      <li>
        <span class="label label-success">05/2025</span>
        Featured in the coverage photo of Guangzhou Nansha.
        æˆ‘ä»¬å®éªŒå®¤è¢«å¹¿å·å—æ²™å®˜åª’é€‰åšå‘å¸ƒå°é¢
        [<a href="https://mp.weixin.qq.com/s/N5ojpgMaWTu1NGYx5dJKbg?scene=1">å‘åŠ›äººå·¥æ™ºèƒ½ï¼Œå—æ²™åˆæœ‰å¤§åŠ¨ä½œ | å¹¿å·å—æ²™å‘å¸ƒ</a>]
      </li>
      <li>
        <span class="label label-info">05/2025</span>
        The University has done a great profile for me.
        å­¦æ ¡å®˜åª’æŠ¥é“ï¼šâ€œæ–œæ é’å¹´â€è¿‡äº”å››ï¼šä»–ä»¬åœ¨æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰è§£é”Nç§äººç”Ÿå‰¯æœ¬ã€‚
        [<a href="https://mp.weixin.qq.com/s/39bhmntjBlD0xAbVOE5rVA">é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰</a>]
      </li>
      <li>
        <span class="label label-success">04/2025</span> é‡å­ä½æŠ¥é“HR-Alignæœºå™¨äººæ“ä½œç ”ç©¶å·¥ä½œï¼ˆCVPR 2025ï¼‰
        [<a href="https://mp.weixin.qq.com/s/4D58UpJ7g3gmQbRyx0bCwQ">è®©æœºå™¨äººæ›´åƒäººç±»æœ‰äº†æ–°æ–¹æ³•ï¼æ¸¯ç§‘å¤§æ–°ç®—æ³•å¯¹é½äººæœºåŠ¨ä½œå½¢æ€ï¼Œæ— éœ€é‡å¤è®­ç»ƒï¼Œè½»é‡æ¨¡å—é€šç”¨ä¸”é€‚é…</a>]
        [<a href="https://jiaming-zhou.github.io/projects/HumanRobotAlign/">é¡¹ç›®ç½‘ç«™</a>]
        [<a href="https://zhuanlan.zhihu.com/p/1906386987004434137">çŸ¥ä¹</a>]
      </li>
      <li>
        <span class="label label-success">04/2025</span> é‡å­ä½æŠ¥é“FALCONç¤¾äº¤å¯¼èˆªç ”ç©¶å·¥ä½œï¼ˆICRA 2025ï¼‰
        [<a href="https://mp.weixin.qq.com/s/NT8Wk6N3S_mQwN_f39Arog">è®©æœºå™¨äººåœ¨äººç¾¤ä¸­ç©¿æ¢­è‡ªå¦‚ï¼Œæ¸¯ç§‘å¹¿&æ¸¯ç§‘å¤§çªç ´ç¤¾äº¤å¯¼èˆªç›²åŒº | ICRA 2025</a>]
        [<a href="https://zhuanlan.zhihu.com/p/1890381093804426351?utm_medium=social&utm_psn=1890423451568821240">çŸ¥ä¹</a>]
        [<a href="https://mp.weixin.qq.com/s/iKeJJKG4QGReN0RYUvJKEA">å­¦æ ¡åª’ä½“æŠ¥é“</a>]
      </li>
      <li>
        <span class="label label-success">03/2025</span>
        è·å¾—â€œAI100 é’å¹´å…ˆé”‹â€å¥– by MIT Technology Review
        [<a href="https://mp.weixin.qq.com/s/cxz0GDor7HqFuooPVyhu8Q">éº»çœç†å·¥ç§‘æŠ€è¯„è®º</a>]
        [<a href="https://mp.weixin.qq.com/s/pCatQkI2PY_utv_aPnJvQw">DeepTech</a>]
        [<a href="https://www.xiaohongshu.com/explore/67e7a1c6000000001c00e388?xsec_token=ABb9P73rz_xT3vtWs7U47WstMlvIGjtnAG_uEKjrx7XW4=&xsec_source=pc_user">å°çº¢ä¹¦</a>]
        [<a href="https://mp.weixin.qq.com/s/kcQl1R5zZHdC8FuZ8lb1jw">é¦™æ¸¯ç§‘æŠ€å¤§å­¦ä¿¡æ¯æ¢çº½</a>]
      </li>
      <li>
        <span class="label label-success">03/2025</span> æœºå™¨ä¹‹å¿ƒæŠ¥é“SeeGroundç ”ç©¶å·¥ä½œï¼ˆCVPR 2025ï¼‰
        [<a href="https://mp.weixin.qq.com/s/LakS8zqiA5XunmEQykKCDw">CVPR 2025 | Qwenè®©AIã€Œçœ‹è§ã€ä¸‰ç»´ä¸–ç•Œï¼ŒSeeGroundå®ç°é›¶æ ·æœ¬å¼€æ”¾è¯æ±‡3Dè§†è§‰å®šä½</a>]
      </li>
      <li>
        <span class="label label-success">03/2025</span> æ™ºçŒ©çŒ©é‚€è¯·ç›´æ’­ä»‹ç»å®éªŒå®¤ç ”ç©¶å·¥ä½œ
        [<a href="https://mp.weixin.qq.com/s/Yozn7Qn1N7MHOSO0zxVVSw">æ¶‰åŠå¤šç¯‡CVPR 2025ï¼æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰åŠ©ç†æ•™æˆæ¢ä¿Šå«ä¸»è®²é€šç”¨æœåŠ¡å…·èº«æ™ºèƒ½æœºå™¨äººçš„ç ”ç©¶è¿›å±• | è®²åº§é¢„å‘Š</a>]
      </li>
      <li>
        <span class="label label-success">12/2024</span> æ¢“å¸†ã€ç‰¹ç«‹ã€ä½³æ˜è·å¾—2024æ·±åœ³æ™ºèƒ½æœºå™¨äººçµå·§æ‰‹å¤§èµ›ä¼˜èƒœå¥–
        [<a href="https://airs.cuhk.edu.cn/page/1226">æ·±åœ³å¸‚äººå·¥æ™ºèƒ½ä¸æœºå™¨äººç ”ç©¶é™¢ä¸¾åŠ</a>]
      </li>
      <li>
        <span class="label label-success">12/2024</span> è¿ªèªè·å¾—å¹¿æ±½é›†å›¢-æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰åˆ›æ–°äººæ‰å‘å±•å¥–å­¦é‡‘
        [<a href="https://mp.weixin.qq.com/s/kMudpT6nwoI2w5w4CByPjg">é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰</a>]
      </li>

      <li>
        <span class="label label-success">10/2024</span> å¹¿å·å¸‚å§”ä¹¦è®°éƒ­æ°¸èˆªåˆ°é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰è°ƒç ”ï¼Œé›…å¯æ¯”æœºå™¨äººè¿›è¡ŒDemoæ¼”ç¤º
        [<a href="https://mp.weixin.qq.com/s/i-MD6Ok_NgIxFri5ZxdgZA">å¹¿å·æ–°é—»è”æ’­</a>]
      </li>
      <li>
        <span class="label label-success">10/2024</span> å›½å†…å¤–é«˜æ ¡å…·èº«æ™ºèƒ½å®éªŒå®¤ç›˜ç‚¹ï¼ˆé¦™æ¸¯ã€æ–°åŠ å¡ç¯‡ï¼‰
        [<a href="https://mp.weixin.qq.com/s/JeF4OQLNmuyRnNGpfyNmbQ">å…·èº«æ™ºèƒ½ä¹‹å¿ƒ</a>]
      </li>
      <li>
        <span class="label label-success">09/2024</span> â€œä¸‡äº¿â€å…·èº«æ™ºèƒ½çš„å¸ˆå¾’â€œæ±Ÿæ¹–â€
        [<a href="https://mp.weixin.qq.com/s/Sy44-1dJRdhyQpytZxNf5g">ç¡…æ˜ŸäººPro</a>]
      </li>
      <li>
        <span class="label label-success">07/2024</span> åœ¨ä¸–ç•Œäººå·¥æ™ºèƒ½å¤§ä¼šå‘è¡¨â€œé¢å‘é€šç”¨æœåŠ¡çš„å…·èº«æ™ºèƒ½â€æ¼”è®²
        [<a href="https://online2024.worldaic.com.cn/forumdetail?uuid=6e9fca0d377844e085fe7211f300ca19">ä¸Šæµ·WAIC</a>]
        [<a href="https://mp.weixin.qq.com/s/qZsHR-3adiDuku6ynwmcFg">è”æ±‡ç§‘æŠ€</a>]
      </li>
      <li>
        <span class="label label-success">06/2024</span> æ¥å—å¹¿å·æ—¥æŠ¥é‡‡è®¿
        [<a href="https://gzdaily.dayoo.com/pc/html/2024-06/08/content_868_860334.htm">å¹¿å·æ—¥æŠ¥ - çœ‹AIæ—¶ä»£çš„é—®ä¸ç­”</a>]
      </li>
      <li>
        <span class="label label-success">06/2024</span> åœ¨é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰ç¬¬äºŒå±ŠINNOTECHå±•ç¤ºæœºå™¨ç‹—å’Œçµå·§æ‰‹Demo
        [<a href="https://mp.weixin.qq.com/s/iBIPX0iIQXUF4z_6IG2lRA">é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰ç¬¬äºŒå±ŠINNOTECHåˆ›ç§‘å˜‰å¹´åå†åˆ›è¾‰ç…Œ</a>]
      </li>
      <li>
        <span class="label label-success">06/2024</span> ä¸‰æ²™å«è§†é‡‡è®¿
        [<a href="https://www.ssws.tv/p/52195.html">æ‰“é€ å¤§æ¹¾åŒºç§‘åˆ›å“ç‰Œç››ä¼š é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰åˆ›ç§‘å˜‰å¹´åä¸¾åŠ</a>]
      </li>
      <li>
        <span class="label label-success">05/2024</span> WAIC Â· äº‘å¸†å¥–äº”å‘¨å¹´ï¼šAI é’å¹´ï¼Œæ‰§æŒæœªæ¥åå¹´çš„é’¥åŒ™
        [<a href="https://mp.weixin.qq.com/s/PIDdgctm-AFux6UjPh3Vsw">å…¨çƒé«˜æ ¡äººå·¥æ™ºèƒ½å­¦æœ¯è”ç›Ÿ</a>]
      </li>
      <li>
        <span class="label label-success">04/2024</span> å¤§æ¨¡å‹â€œè¶Šç‹±â€ å¦‚ä½•ç›‘ç®¡å¼€å‘è€…
        [åº”é‚€ <a href="https://gzdaily.dayoo.com/h5/html5/2024-04/29/content_878_856859.htm">å¹¿å·æ—¥æŠ¥</a> é‡‡è®¿]
        [HKUST(GZ) under the spotlight (April-May 2024, Issue 1) <a href="https://huacheng.gz-cmc.com/pages/2024/04/26/42405655b18a44a192ff833c0e7538f8.html">(Link)</a>]
      </li>
      <li>
        <span class="label label-success">04/2024</span> ä»–çš„ä»£ç åœ¨NASAä¸Šå¤©ï¼Œåœ¨æ¸¯ç§‘å¹¿è½åœ°
        [by <a href="https://mp.weixin.qq.com/s/pUpWrnDtIm5gK_ZWOWhxHg">HKUST(GZ)</a>]
        [<a href="https://mp.weixin.qq.com/s/WJxtyM2FRzOABdz_N35a6A">INFO Hub</a>]
        [<a href="https://mp.weixin.qq.com/s/5smO2jbZV7_OOXIecxWNPw">AI Thrust</a>]
      </li>
      <li>
        <span class="label label-success">02/2024</span> åäººCMUæ ¡å‹å›å›½åˆ›ä¸šï¼Œè‡ªç ”å…·èº«æ™ºèƒ½æœºå™¨äººï¼Œè‡´åŠ›äºå¼€æ”¾åœºæ™¯çš„å•†ä¸šåŒ–è½åœ°
        [by <a href="https://mp.weixin.qq.com/s/f8NfHPlF208xEVRmtBocZg">DeepTechæ·±ç§‘æŠ€</a>]
      </li>
      <li>
        <span class="label label-success">12/2023</span> é›…å¯æ¯”æœºå™¨äººè·å¾—2023æ–°ä¸€ä»£äººå·¥æ™ºèƒ½ï¼ˆæ·±åœ³ï¼‰åˆ›ä¸šå¤§èµ›ä¸‰ç­‰å¥–
        [by <a href="https://m.163.com/tech/article/IL7I2EOJ00097U7R.html">163 news</a>, <a href="https://mp.weixin.qq.com/s/x9BYlCXb7BTQvJcZMYFnzQ">é›…å¯æ¯”æœºå™¨äºº</a>]
      </li>
      <li>
        <span class="label label-success">10/2023</span> Patchæ‰æ˜¯æ—¶åºé¢„æµ‹çš„ç‹é“ï¼Ÿ
        [by <a href="https://mp.weixin.qq.com/s/JBO7_xyMwsATO_zi4LWbzQ">åœ†åœ†çš„ç®—æ³•ç¬”è®°</a>, <a href="https://mp.weixin.qq.com/s/ElRBgAWTveItXJSoZe2DLQ">kaggleç«èµ›å®å…¸</a>]
        [<a href="https://browse.arxiv.org/pdf/2310.00655v1.pdf">Paper</a>]
      </li>
      <li>
        <span class="label label-success">10/2023</span> TFNetï¼šåˆ©ç”¨æ—¶é—´çº¿ç´¢å®ç°å¿«é€Ÿä¸”ç²¾ç¡®çš„æ¿€å…‰é›·è¾¾è¯­ä¹‰åˆ†å‰²
        [by <a href="https://mp.weixin.qq.com/s/V5uuTEwC8gFepIdXzFYAxA">è‡ªåŠ¨é©¾é©¶ä¸“æ </a>]
        [<a href="https://arxiv.org/pdf/2309.07849.pdf">Paper</a>]
      </li>
      <li>
        <span class="label label-success">06/2023</span> Honorable Mention at the International Robot Manipulation Competition, Robothon (2023æœºå™¨äººé©¬æ‹‰æ¾æŒ‘æˆ˜èµ›)
        [by <a href="https://mp.weixin.qq.com/s/V5uuTEwC8gFepIdXzFYAxA">é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰å…¬ä¼—å·</a>]
        [<a href="https://automatica-munich.com/en/munich-i/robothon/teams/#team_submissions">Robothon 2023</a>]
      </li>
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="news" href="#news">News</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="label label-info">02/2026</span>
        Our first-person trajectory prediction research (EgoTraj-Bench) has been accepted by ICRA 2026
        [<a href="https://zoeyliu1999.github.io/EgoTrajBench/">Project page</a>]
      </li>
      <li>
        <span class="label label-info">01/2026</span> ASCENT is accepted by IEEE Robotics and Automation Letters (RA-L)ï¼ˆæœºå™¨äººæ–¹å‘é¡¶åˆŠï¼‰!
        [<a href="https://zeying-gong.github.io/projects/ascent/">Project Page</a>]
        [<a href="https://arxiv.org/abs/2505.23019">Paper</a>]
        [<a href="https://github.com/Zeying-Gong/ascent">Code and model</a>]
        [<a href="https://mp.weixin.qq.com/s/kg4c9H_tS-I4xoJaOW-dXg">RAL-2026 | æ¸¯ç§‘å¹¿å¤šæ¥¼å±‚æœºå™¨äººå¯¼èˆªæ–°çªç ´ï¼ASCENT: å®ç°æ¥¼å±‚æ„ŸçŸ¥çš„é›¶æ ·æœ¬ç‰©ä½“ç›®æ ‡å¯¼èˆª</a>]
      </li>
      <li>
        <span class="label label-info">12/2025</span> Our Self-driving VLA Survey is released and reported by others!
        [<a href="https://mp.weixin.qq.com/s/ZZnYSzwJTVDwb-QQpv7xgw">ã€VLAç»¼è¿°ã€‘æ¸¯ç§‘æ²ˆåŠ­åŠ¼æ¢ä¿Šå«NTUåˆ˜å­çº¬ç­‰å›¢é˜Ÿè‡ªåŠ¨é©¾é©¶VLAé‡ç£…ç»¼è¿°ï¼</a>]
        [<a href="https://mp.weixin.qq.com/s/5bkaJCY4NCgsh9oe33NmmQ">æ·±è“AIæŠ¥é“</a>]
        [<a href="https://arxiv.org/abs/2512.16760">Paper</a>]
      </li>
      <li>
        <span class="label label-info">12/2025</span> åœ¨å¤§æ¹¾åŒºç§‘å­¦è®ºå›-å…·èº«æ™ºèƒ½åˆ†è®ºå›è¿›è¡Œä¸»é¢˜æ¼”è®²å¹¶æ¼”ç¤ºå®éªŒå®¤Demo
        [<a href="https://mp.weixin.qq.com/s/4QT0VK9muqHCpobvZ3pHBA">æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰ä¸»åŠï¼2025å¤§æ¹¾åŒºç§‘å­¦è®ºå›å…·èº«æ™ºèƒ½åˆ†è®ºå›éš†é‡ä¸¾è¡Œ</a>]
        [<a href="https://www.xiaohongshu.com/explore/69342096000000001d039dca?xsec_token=AB-TJgirWrcOexYY3-m5D0DxRkncoe3hzQLZdI8h7vFQ0=&xsec_source=pc_user">å°çº¢ä¹¦</a>]
      </li>
      <li>
        <span class="label label-info">10/2025</span> 21ä¸–çºªè´¢ç»ä¸“è®¿
        [<a href="https://m.21jingji.com/article/20251208/herald/7d0d4108c457bb3abf676a1734beb3d6.html?t=772148">â€œæ¢ä¿Šå«ï¼šç»™å¤§æ¨¡å‹å®‰è£…â€œä¸€åŒæ‰‹â€è¦ç»å†å‡ æ­¥ï¼Ÿâ€</a>]
        [<a href="https://www.bilibili.com/video/BV1qV2SBSEoi/">Bç«™</a>]
      </li>
      <li>
        <span class="label label-info">09/2025</span>
        GLOVER accepted at CoRL 2025 GenPriors Workshop and received <span style="font-weight:bold;color:red">Best Paper award</span>
        [<a href="https://teleema.github.io/projects/GLOVER/">Project page</a>]
        [<a href="https://x.com/JunweiLiangCMU/status/1972109244108623987">Twitter</a>]
        [<a href="https://www.xiaohongshu.com/explore/68d7a6a00000000012030773?app_platform=ios&app_version=9.1.2&share_from_user_hidden=true&xsec_source=app_share&type=normal&xsec_token=CBiPKCZngxg470muJsud7g2j4JtmYQtJ35q8GndUH3grk=&author_share=1&xhsshare=WeixinSession&shareRedId=ODhFM0Q8PDw2NzUyOTgwNjdFOTlGNT9C&apptime=1758963375&share_id=c61d61af7751472aa32764b87b05d9db">å°çº¢ä¹¦</a>]
      </li>
      <li>
        <span class="label label-info">09/2025</span>
        Two papers accepted at <span style="font-weight:bold;">NeurIPS 2025</span>.
        [<a href="https://jiaming-zhou.github.io/AGNOSTOS/">Cross-task manipulation benchmark, AGNOSTOS</a>]
        [<a href="https://project-3eed.github.io/">Ground Everything Everywhere in 3D</a>]
      </li>
      <li>
        <span class="label label-info">08/2025</span>
        Two papers accepted at CoRL 2025 <span style="font-weight:bold;color:red">(5%å½•å–ç‡ï¼Œå¤§æ¹¾åŒºå”¯ä¸€çš„Oral paper)</span>
        [<a href="https://acodedog.github.io/OmniPerceptionPages/">LiDAR-based Locomotion Policy</a>]
        [<a href="https://mp.weixin.qq.com/s/7gyV4iSF1G4sSyuoDRRMDg">æ·±è“å…·èº«æ™ºèƒ½æŠ¥é“</a>]
        [<a href="https://teleema.github.io/projects/GLOVER++/">GLOVE++: Open-Vocab Affordance</a>]
        [<a href="https://mp.weixin.qq.com/s/r9JvBsPf9jBQqiRe2UXVgQ?scene=1">æ™ºçŒ©çŒ©Robotè§£è¯»</a>]
      </li>
      <li>
        <span class="label label-info">07/2025</span>
        Make a humanoid robot demo with manipulation for the university.
        æ”¯æŒæ¸¯ç§‘å¹¿2025çº§æœ¬ç§‘ç”Ÿå½•å–æ´»åŠ¨ï¼Œäººå½¢æœºå™¨äººé€’é€å½•å–é€šçŸ¥ä¹¦
        [<a href="https://mp.weixin.qq.com/s/jIJfb1G1MPbs-GTfKwNhCQ?scene=1">ç§‘æŠ€èµ‹èƒ½è¿æ–° æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰ä¸¾åŠæœ¬ç§‘æ–°ç”Ÿä»£è¡¨è§é¢æ´»åŠ¨</a>]
        [å¾®ä¿¡è§†é¢‘å·ï¼šé¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰- ç›´å‡»æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰ç¡¬æ ¸æœ¬ç§‘æ–°ç”Ÿä»£è¡¨è§é¢ä¼šç°åœºï¼]
        [<a href="https://www.xiaohongshu.com/explore/687c52b5000000000b02d825?source=webshare&xhsshare=pc_web&xsec_token=ABrVSHQdRMREeTYR1Q0zKAOpzcRqmEAnJF5Rf1SCtAEvk=&xsec_source=pc_share">Demo Video - å°çº¢ä¹¦</a>]
        [å¾®ä¿¡è§†é¢‘å·ï¼šç¾ŠåŸæ™šæŠ¥ - æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰æœ¬ç§‘æ–°ç”Ÿä»£è¡¨è§é¢ä¼šï¼šäººå½¢æœºå™¨äººé€è¾¾å½•å–é€šçŸ¥ä¹¦]
        [å¾®ä¿¡è§†é¢‘å·ï¼šå¹¿å·å¹¿æ’­ç”µè§†å° - ä½ çš„æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰å½•å–é€šçŸ¥ä¹¦ï¼Œäººå½¢æœºå™¨äººåœ¨å¼€ç€â€œç–¾è·‘ â€é€æ¥ï¼]
      </li>
      <li>
        <span class="label label-info">07/2025</span>
        One paper accepted at ICCV 2025
        [<a href="https://gaussian-property.github.io/">Robot Manipulation with Physical-Property 3D Gaussians</a>]
        [<a href="https://arxiv.org/abs/2412.11258">paper</a>]
      </li>
      <li>
        <span class="label label-info">07/2025</span>
        ä½œä¸ºæ¼”è®²å˜‰å®¾å‚åŠ ä¸­å›½ç©ºé—´æ™ºèƒ½å¤§ä¼š @æ·±åœ³
        [<a href="https://mp.weixin.qq.com/s/mEddG_mbKFRarp0xetszGw?scene=1">ä¸­å›½ç©ºé—´æ™ºèƒ½å¤§ä¼š</a>]
      </li>
      <li>
        <span class="label label-info">06/2025</span>
        åœ¨æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰INNOTECHæ´»åŠ¨ç°åœºæ¼”ç¤ºæœ€æ–°locomotionã€äººå½¢æœºå™¨äººç§‘ç ”æˆæœã€‚
        Presented Live Demos at INNOTECH 2025@HKUST-GZ.
        [<a href="https://mp.weixin.qq.com/s/7XNbaXuVAi2WI2HRrt6GWg?scene=1">å¹¿å·å—æ²™å‘å¸ƒ</a>]
        [<a href="https://mp.weixin.qq.com/s/LvXlZdRN2Se95bAwxElNrw?scene=1">é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰</a>]
      </li>
      <li>
        <span class="label label-info">06/2025</span>
        We are organizing a Social Navigation Competition at IROS 2025.
        æˆ‘ä»¬æ­£åœ¨ä¸¾åŠIROS 2025æœºå™¨äººç¤¾äº¤å¯¼èˆªæ¯”èµ›ï¼Œæ¬¢è¿å‚èµ›ï¼åæœˆæ­å·è§ï¼
        [<a href="https://robosense2025.github.io/#organizers">website æ¯”èµ›ç½‘ç«™</a>]
        [<a href="https://x.com/JunweilLiang/status/1930881503636127887">Tweet</a>]
        [<a href="https://www.xiaohongshu.com/discovery/item/68423902000000002202801a?source=webshare&xhsshare=pc_web&xsec_token=ABSybfxl1fFiqIS3y8k-ka_SSRaJMuSZbvX3mBfjhWbp8=&xsec_source=pc_share">å°çº¢ä¹¦å®£ä¼ </a>]
        [<a href="https://zhuanlan.zhihu.com/p/1914310707429737712">çŸ¥ä¹</a>]
      </li>
      <li>
        <span class="label label-info">06/2025</span>
        ä½œä¸ºé’å¹´è€å¸ˆå˜‰å®¾å‚åŠ VALSE 2025ä¼˜ç§€å­¦ç”Ÿè®ºå› @ç æµ·
        [<a href="https://mp.weixin.qq.com/s/E9PyUWEiNNuQ9MmCPmYfMw?scene=1">VALSE 2025ä¸“é¢˜è®ºå› | ä¼˜ç§€å­¦ç”Ÿè®ºå›ï¼šå‰æ²¿æŠ¥å‘Š+ä¸»é¢˜è¾©è®º+è±ªåå¯¼å¸ˆé¢å¯¹é¢</a>]
      </li>
      <li>
        <span class="label label-info">04/2025</span>
        Present a talk on Embodied AI for the university.
        èšç„¦å‰æ²¿ï¼Œå…±è¯æœªæ¥ ï¼šæ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰å…·èº«æ™ºèƒ½ç³»åˆ—è®ºå›é¦–åœºåœ¨å—æ²™IFCä¸¾è¡Œ
        [<a href="https://mp.weixin.qq.com/s/QCHTQZ0a67oARhaiu1n0xw">æ´»åŠ¨è®°å½•</a>]
      </li>
      <li>
        <span class="label label-info">04/2025</span>
        Serving as <span style="font-weight: bold;">Area Chair</span> for NeurIPS 2025.
      </li>
      <li>
        <span class="label label-info">03/2025</span>
        Present a general Embodied AI introduction lecture for the university.
        åœ¨æ ¡å†…ä¸¾è¡Œäº†å…·èº«æ™ºèƒ½å‰æ²¿åº”ç”¨è®²åº§ï¼Œç»™å¤–è¡Œã€éæŠ€æœ¯äººå‘˜ä»‹ç»å…·èº«æ™ºèƒ½çš„ç›¸å…³æ¦‚å¿µä¸å‰æ²¿åº”ç”¨ï¼Œåå“ä¸é”™
        [<a href="https://mp.weixin.qq.com/s/7Qb_pvlvp3sX7uhWy56vWw">å®£ä¼ é¢„å‘Š</a>]
        [<a href="https://hkust-gz-edu-cn.zoom.us/rec/share/cGrIsDPi4THbWFz6wZgB17v1d601AgaJX7DwxFg3UvaHV0VkHBnLb6TgqkOYiHN3.w-SuMt6DszL4LrDd">è®²åº§å½•åƒ</a>]
        [<a href="https://docs.qq.com/sheet/DRHFqVXNmend5TVBJ?tab=000001">è®²åº§æ»¡æ„åº¦</a>]
      </li>
      <li>
        <span class="label label-info">02/2025</span>
        Two papers accepted at CVPR 2025.
        [<a href="https://jiaming-zhou.github.io/projects/HumanRobotAlign/">Robot Manipulation</a>]
        [<a href="https://seeground.github.io/">3D Visual Grounding</a>]
        [<a href="https://mp.weixin.qq.com/s/4D58UpJ7g3gmQbRyx0bCwQ">é‡å­ä½æŠ¥é“</a>]
        [<a href="https://mp.weixin.qq.com/s/LakS8zqiA5XunmEQykKCDw">æœºå™¨ä¹‹å¿ƒæŠ¥é“</a>][<a href="https://www.zhihu.com/question/13320524361/answer/130587951372">çŸ¥ä¹</a>]
        [<a href="https://mp.weixin.qq.com/s/Yozn7Qn1N7MHOSO0zxVVSw">æ™ºçŒ©çŒ©ç›´æ’­</a>]
      </li>
      <li>
        <span class="label label-info">01/2025</span>
        One paper accepted at ICRA 2025.
        [<a href="https://mp.weixin.qq.com/s/NT8Wk6N3S_mQwN_f39Arog">é‡å­ä½æŠ¥é“</a>]
        [<a href="https://zeying-gong.github.io/projects/falcon/">Social Navigation</a>]
        [<a href="https://mp.weixin.qq.com/s/iKeJJKG4QGReN0RYUvJKEA">å­¦æ ¡åª’ä½“æŠ¥é“</a>]
        [<a href="https://zhuanlan.zhihu.com/p/20569173162">çŸ¥ä¹</a>]
      </li>
      <li>
        <span class="label label-success">12/2024</span> æ¢“å¸†ã€ç‰¹ç«‹ã€ä½³æ˜è·å¾—2024æ·±åœ³æ™ºèƒ½æœºå™¨äººçµå·§æ‰‹å¤§èµ›ä¼˜èƒœå¥–
        [<a href="https://airs.cuhk.edu.cn/page/1226">æ·±åœ³å¸‚äººå·¥æ™ºèƒ½ä¸æœºå™¨äººç ”ç©¶é™¢ä¸¾åŠ</a>]
      </li>
      <li>
        <span class="label label-success">12/2024</span> è¿ªèªè·å¾—å¹¿æ±½é›†å›¢-æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰åˆ›æ–°äººæ‰å‘å±•å¥–å­¦é‡‘ (20ä¸‡)
        [<a href="https://mp.weixin.qq.com/s/kMudpT6nwoI2w5w4CByPjg">é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰</a>]
      </li>
      <li>
        <span class="label label-info">11/2024</span>
        Presented "Towards General Service Embodied AI" at ARTS 2024. [<a href="https://mp.weixin.qq.com/s/dns8-rn1XAABrSdFJ-Aq7A">è‡ªä¸»æœºå™¨äººæŠ€æœ¯ç ”è®¨ä¼š</a>]
      </li>
      <li>
        <span class="label label-info">11/2024</span>
        My first PhD student, <a href="https://zgzxy001.github.io/">Xiaoyu Zhu</a>, has successfully defended her thesis and will graduate from CMU. Congrats to Xiaoyu! [<a href="https://www.cs.cmu.edu/calendar/178568916">Learning Generalizable Visual Representations Towards Novel Viewpoints, Scenes and Vocabularies</a>]
      </li>
      <li>
        <span class="label label-info">10/2024</span>
        Presented "Towards General Service Embodied AI" at Huawei and CCF-YOCSEF seminar.
      </li>
      <li>
        <span class="label label-info">09/2024</span>
        Presented "Towards General Service Embodied AI" at CCF/CSIG GAMES Seminar.
        [<a href="https://mp.weixin.qq.com/s/Vi7_VfHk8kE8XD_JEEQU3Q">ç¬¬ä¹å±Šè®¡ç®—æœºå›¾å½¢å­¦ä¸æ··åˆç°å®ç ”è®¨ä¼š</a>]
      </li>
      <li>
        <span class="label label-info">09/2024</span>
        One paper accepted at <a href="https://teleema.github.io/projects/Sigma_Agent/">CoRL 2024</a>.
      </li>
      <li>
        <span class="label label-info">09/2024</span>
        One paper accepted at NeurIPS 2024.
      </li>
      <li>
        <span class="label label-info">08/2024</span>
        2 papers accepted at IROS 2024.
      </li>
      <li>
        <span class="label label-info">09/2024</span>
        Presented "Towards General Service Embodied AI" at CAA's Seminar.
        [<a href="https://mp.weixin.qq.com/s/JNbY0mDoq4v6ugGL6LNiJQ">CAAä¸­å›½è‡ªåŠ¨åŒ–å­¦ä¼šäº‘è®²åº§</a>]
      </li>
      <li>
        <span class="label label-info">08/2024</span>
        Presented "Towards General Service Embodied AI" at CAAI's Embodied AI Seminar.
        [<a href="https://mp.weixin.qq.com/s/MkirFYZJYJu-tZgMRaSE-w">CAAIä¸­å›½äººå·¥æ™ºèƒ½å­¦ä¼šå…·èº«æ™ºèƒ½é’å¹´å­¦è€…ç ”è®¨ä¼šç¬¬äº”æœŸ</a>]
        [<a href="https://www.zhihu.com/zvideo/1815073015802781696">Video Recording</a>]
      </li>
      <li>
        <span class="label label-info">07/2024</span>
        Two papers accepted at ECCV 2024.
      </li>
      <li>
        <span class="label label-info">05/2024</span>
        1 paper accepted at <a href="https://teleema.github.io/projects/SADE/sade.html">NAACL 2024</a>.
        1 paper accepted at ACL 2024. Main conference.
      </li>
      <li>
        <span class="label label-info">02/2024</span>
        Serve as a panelist at the VALSE Embodied AI webinar.
        [<a href="https://mp.weixin.qq.com/s/788DpNUbjklH-unhznkewg?poc_token=HJlaAmaj-hqsfo_KezM6IrqyY7MsZZ5wYfVpxEyz">VALSE</a>]
        [<a href="https://live.bilibili.com/22300737">bilibili</a>]

      </li>
      <li>
        <span class="label label-info">02/2024</span> Co-organizing the <a href="https://sites.google.com/view/ieeecvf-cvpr2024-precognition">The 6th workshop on Precognition: Seeing through the Future</a> @CVPR 2024.
        [<a href="https://www.linkedin.com/posts/junweiliang_cvpr2024-computervision-visionforecasting-activity-7163724230201733120-ERWa/">Call For Papers</a>] [<a href="https://zhuanlan.zhihu.com/p/682320005">çŸ¥ä¹</a>] [<a href="https://www.xiaohongshu.com/explore/65cd76e400000000070055a8">å°çº¢ä¹¦</a>]
      </li>
      <li>
        <span class="label label-info">12/2023</span> Keynote speech at the CEII2023 Workshop
        [<a href="https://mp.weixin.qq.com/s/FRZ_r2BrvCz2Y7RreJdLgA">Schedule</a>]
      </li>
      <li>
        <span class="label label-info">10/2023</span> Co-organizing the Open-world Visual Perception Workshop (â€œå¼€æ”¾ä¸–ç•Œä¸‹çš„è§†è§‰æ„ŸçŸ¥å’Œå¢å¼ºâ€ä¸»é¢˜è®ºå›) @PRCV 2023
        [<a href="https://mp.weixin.qq.com/s/ib9aKBhQhoaAFqZB93F3wQ">Schedule</a>]
      </li>
      <li>
        <!--<a href="https://www.linkedin.com/posts/khoa-luu-90900215_cvpr2023-activity-7009293998955655168-M4lu?utm_source=share&utm_medium=member_desktop">Precognition workshop</a>-->
        <span class="label label-info">01/2023</span> Co-organizing the <a href="https://sites.google.com/view/ieeecvf-cvpr2023-precognition">The 5th workshop on Precognition: Seeing through the Future</a> @CVPR 2023.
        [<a href="https://www.linkedin.com/posts/junweiliang_cvpr2023-workshop-computervision-activity-7030466054787121152-NacF?utm_source=share&utm_medium=member_desktop">Call For Papers</a>] [<a href="https://zhuanlan.zhihu.com/p/603134088">çŸ¥ä¹</a>]
      </li>
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="members" href="#members">Lab Members</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <a href="https://teleema.github.io/">Teli Ma</a>
        [<a href="https://github.com/TeleeMa">Github</a>]
        [<a href="https://scholar.google.com/citations?user=arny77IAAAAJ">Google Scholar</a>]
        [<a href="https://www.zhihu.com/people/yin-he-xi-xiao-wang-zi">çŸ¥ä¹</a>]
      </li>

      <li>
        <a href="https://jiaming-zhou.github.io/">Jiaming Zhou</a>
        [<a href="https://github.com/jiaming-zhou/">Github</a>]
        [<a href="https://scholar.google.com/citations?user=b3y40w8AAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="https://zifanw.notion.site/">Zifan Wang</a>
        [<a href="https://github.com/aCodeDog">Github</a>]
        [<a href="https://scholar.google.com/citations?user=GaJXZ-UAAAAJ&hl=zh-CN">Google Scholar</a>]
      </li>

      <li>
        <a href="http://www.davidqiu.com/">Dicong Qiu</a>
        [<a href="https://github.com/davidqiu1993">Github</a>]
        [<a href="https://scholar.google.com/citations?user=ZFmSow8AAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="https://openreview.net/profile?id=~Ronghe_Qiu2">Ronghe Qiu</a>
        [<a href="https://github.com/ConnerQiu">Github</a>]
        [<a href="https://scholar.google.com/citations?hl=en&user=QOwOHZMAAAAJ">Google Scholar</a>]
      </li>
      <li>
        <a href="https://zeying-gong.github.io/">Zeying Gong</a>
        [<a href="https://github.com/Zeying-Gong">Github</a>]
        [<a href="https://scholar.google.com/citations?user=ze2Wh9EAAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="https://rongli.tech/">Rong Li</a>
        [<a href="https://www.zhihu.com/people/Iris0329">çŸ¥ä¹</a>]
        [<a href="https://github.com/iris0329">Github</a>]
        [<a href="https://scholar.google.com/citations?user=M68wBgkAAAAJ">Google Scholar</a>]
      </li>
      <li>
        <a href="https://hutslib.github.io/">Tianshuai Hu</a>
        [<a href="https://github.com/hutslib">Github</a>]
        [<a href="https://scholar.google.com.hk/citations?user=RJ7NR54AAAAJ&hl=zh-CN">Google Scholar</a>]
      </li>
      <li>
        <a href="https://www.jiayi-liu.cn/">Jiayi Liu</a>
        [<a href="https://github.com/juceyj">Github</a>]
      </li>

      <hr/>

      Alumni.

      <li>
        <a href="https://scholar.google.com/citations?user=bRq33Q8AAAAJ">Sheng Wang</a>
        (Graduated with Ph.D. @HKUST, co-advised)
        [<a href="https://scholar.google.com/citations?user=bRq33Q8AAAAJ">Google Scholar</a>]
        [<a href="https://www.researchgate.net/profile/Wang-Sheng-41">Research Gate</a>]
      </li>

      <li>
        <a href="https://zgzxy001.github.io/">Xiaoyu Zhu</a>
        (Graduated with Ph.D. @CMU, co-advised. Now at Apple)
        [<a href="https://github.com/zgzxy001">Github</a>]
        [<a href="https://twitter.com/XiaoyuZhu10">Twitter</a>]
        [<a href="https://scholar.google.com/citations?user=T4Dc5zEAAAAJ&hl=en">Google Scholar</a>]
      </li>

      <li>
        <a href="https://jhuiye.com/">Jinhui Ye</a>
        (Visiting @CMU) (Now PhD of Jiaya Jia@HKUST)
        [<a href="https://github.com/JinhuiYE">Github</a>]
        [<a href="https://scholar.google.com/citations?user=RpXhJu0AAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="http://www.linkedin.com/in/jian-chen-a7501517b">Jian Chen</a>
        (Now with HSBC and pursuing a PhD)
        [<a href="https://scholar.google.com/citations?user=fmPehrUAAAAJ&hl=en&oi=sra">Google Scholar</a>]
        [<a href="https://github.com/AlexJJJChen">Github</a>]
      </li>
      <li>
        <a href="https://yyyujintang.github.io/">Yujin Tang</a>
        (Visited <a href="https://scholar.google.com/citations?user=p9-ohHsAAAAJ&hl=en">Ming-Hsuan Yang</a>)
        (Now PhD at Dartmouth College)
        [<a href="https://www.zhihu.com/people/30-76-66-6">çŸ¥ä¹</a>]
        [<a href="https://github.com/yyyujintang">Github</a>]
      </li>
      <li>
        <a href="https://github.com/XinyuSun">Xinyu Sun</a>
        (Now at DJI)
        [<a href="https://github.com/XinyuSun">Github</a>]
        [<a href="https://scholar.google.com/citations?user=ALq8sMgAAAAJ">Google Scholar</a>]
      </li>

    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="publications" href="#publications">Publications</a>
  </div>

  <div class="content publications">
    * indicates corresponding authors.
    <ol>
      <li>
        <div class="imgblock"><img src="camera_ready/ascent.png"></img></div>
        <span class="title">Stairway to Success: Zero-Shot Floor-Aware Object-Goal Navigation via LLM-Driven Coarse-to-Fine Exploration
        </span>
        <div class="info text-success italic">
        Zeying Gong, Rong Li, Tianshuai Hu, Ronghe Qiu, Lingdong Kong, Lingfeng Zhang, Yiyi Ding, Leying Zhang, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info">
          <span class="label label-info">IEEE Robotics and Automation Letters (RA-L) 2026</span>
          <iframe src="https://ghbtns.com/github-btn.html?user=Zeying-Gong&repo=ascent&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>

        <div class="stuff">
          [<a href="https://arxiv.org/abs/2505.23019">Paper</a>]
          [<a href="https://zeying-gong.github.io/projects/ascent/">Project page</a>]
          [<a href="https://github.com/Zeying-Gong/ascent">Code</a>]
          [<a href="https://mp.weixin.qq.com/s/kg4c9H_tS-I4xoJaOW-dXg">RAL-2026 | æ¸¯ç§‘å¹¿å¤šæ¥¼å±‚æœºå™¨äººå¯¼èˆªæ–°çªç ´ï¼ASCENT: å®ç°æ¥¼å±‚æ„ŸçŸ¥çš„é›¶æ ·æœ¬ç‰©ä½“ç›®æ ‡å¯¼èˆª</a>]
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/AGNOSTOS.png"></img></div>
        <span class="title">Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization
        </span>
        <div class="info text-success italic">
        Jiaming Zhou, Ke Ye, Jiayi Liu, Teli Ma, Zifan Wang, Ronghe Qiu, Kun-Yu Lin, Zhilin Zhao, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info">
          <span class="label label-info">NeurIPS 2025</span>
          <iframe src="https://ghbtns.com/github-btn.html?user=jiaming-zhou&repo=X-ICM&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2505.15660">Paper</a>]
          [<a href="https://jiaming-zhou.github.io/AGNOSTOS/">Project page</a>]
          [<a href="https://youtu.be/5MKlijK1gKI">Video</a>]
          [<a href="https://github.com/jiaming-zhou/X-ICM">Code</a>]
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/glover++.png"></img></div>
        <span class="title">GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation
        </span>
        <div class="info text-success italic">
        Teli Ma, Jia Zheng, Zifan Wang, Ziyao Gao, Jiaming Zhou, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info">
          <span class="label label-info">CoRL 2025</span>
          <iframe src="https://ghbtns.com/github-btn.html?user=TeleeMa&repo=GLOVER&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2505.11865">Paper</a>]
          [<a href="https://teleema.github.io/projects/GLOVER++/">Project page</a>]
          [<a href="https://www.youtube.com/embed/MDQccK681-k">Video</a>]
          [<a href="https://github.com/TeleeMa/GLOVER">Code</a>]
          [<a href="https://zhuanlan.zhihu.com/p/1941611011254759760">çŸ¥ä¹</a>]
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/glover.gif"></img></div>
        <span class="title">GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for Task-Oriented Grasping
        </span>
        <div class="info text-success italic">
        Teli Ma, Zifan Wang, Jiaming Zhou, Mengmeng Wang, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">CoRL 2025 GenPriors Workshop</span>
          <span class="text-error">ğŸ¥‡Best Paper Award</span>
          <iframe src="https://ghbtns.com/github-btn.html?user=TeleeMa&repo=GLOVER&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">

          [<a href="https://arxiv.org/pdf/2411.12286v1">Paper</a>]
          [<a href="https://teleema.github.io/projects/GLOVER/">Project page</a>]
          [<a href="https://github.com/TeleeMa/GLOVER">Code</a>]
          [<a href="https://x.com/JunweiLiangCMU/status/1972109244108623987">Twitter</a>]
        </div>
        <div style="clear:both"></div>
      </li>


      <li>
        <div class="imgblock"><img src="camera_ready/OmniPerception.png"></img></div>
        <span class="title">Omni-Perception: Omnidirectional Collision Avoidance for Legged Locomotion in Dynamic Environments
        </span>
        <div class="info text-success italic">
        Zifan Wang, Teli Ma, Yufei Jia, Xun Yang, Jiaming Zhou, Wenlong Ouyang, Qiang Zhang, <span style="font-weight:bold">Junwei Liang*</span>
        </div>

        <div class="info"><span class="label label-info">CoRL 2025</span>
          <span class="text-error">(Oral, ~5% acceptance rate)</span>
          <iframe src="https://ghbtns.com/github-btn.html?user=aCodeDog&repo=OmniPerception&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>

        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2505.19214">Paper</a>]
          [<a href="https://acodedog.github.io/OmniPerceptionPages/">Project page</a>]
          [<a href="https://youtu.be/MBbCuaWwXfM">Video</a>]
          [<a href="https://github.com/aCodeDog/OmniPerception">Code</a>]
          [<a href="https://mp.weixin.qq.com/s/7gyV4iSF1G4sSyuoDRRMDg">æ·±è“å…·èº«æ™ºèƒ½æŠ¥é“</a>]
          [<a href="https://mp.weixin.qq.com/s/TM6Js3wAn1jai2oUyyo30Q?scene=1">æ·±è“å…·èº«æ™ºèƒ½è§£è¯»</a>]
          [<a href="https://mp.weixin.qq.com/s/Pogp-uB8Int6U5ECf9i7YQ?scene=1">å…·èº«æ™ºèƒ½ç ”ç©¶å®¤è§£è¯»</a>]
        </div>
        <div style="clear:both"></div>
      </li>


      <li>
        <div class="imgblock"><img src="camera_ready/3eed.png"></img></div>
        <span class="title">3EED: Ground Everything Everywhere in 3D
        </span>
        <div class="info text-success italic">
        Rong Li, Yuhao Dong, Tianshuai Hu, Ao Liang, Youquan Liu, Dongyue Lu, Liang Pan, Lingdong Kong, <span style="font-weight:bold">Junwei Liang*</span>, Ziwei Liu*
        </div>
        <div class="info">
          <span class="label label-info">NeurIPS 2025</span>
          <iframe src="https://ghbtns.com/github-btn.html?user=iris0329&repo=3eed&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          [<a href="https://project-3eed.github.io/">Project page</a>]
          [<a href="https://huggingface.co/datasets/3EED/3EED">Dataset</a>]
          [<a href="https://github.com/iris0329/3eed">Code</a>]
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/jiaming01.png"></img></div>
        <span class="title">Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation
        </span>
        <div class="info text-success italic">
          Jiaming Zhou, Teli Ma, Kun-Yu Lin, Ronghe Qiu, Zifan Wang, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">CVPR 2025</span>
          <iframe src="https://ghbtns.com/github-btn.html?user=jiaming-zhou&repo=HumanRobotAlign&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/pdf/2406.14235">Paper</a>]
          [<a href="https://jiaming-zhou.github.io/projects/HumanRobotAlign/">Project page</a>]
          [<a href="https://mp.weixin.qq.com/s/4D58UpJ7g3gmQbRyx0bCwQ">é‡å­ä½æŠ¥é“</a>]
          [<a href="https://zhuanlan.zhihu.com/p/1906386987004434137">çŸ¥ä¹</a>]
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/seeground.png"></img></div>
        <span class="title">SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding
        </span>
        <div class="info text-success italic">
        Rong Li, Shijie Li, Lingdong Kong, Xulei Yang, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">CVPR 2025</span>
           <iframe src="https://ghbtns.com/github-btn.html?user=iris0329&repo=SeeGround&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2412.04383">Paper</a>]
          [<a href="https://seeground.github.io/">Project page</a>]
          [<a href="https://github.com/iris0329/SeeGround">Code</a>]
          [<a href="https://mp.weixin.qq.com/s/LakS8zqiA5XunmEQykKCDw">æœºå™¨ä¹‹å¿ƒæŠ¥é“</a>]
          [<a href="https://www.zhihu.com/question/13320524361/answer/130587951372">çŸ¥ä¹<a>]
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/zeying01.gif"></img></div>
        <span class="title">From Cognition to Precognition: A Future-Aware Framework for Social Navigation
        </span>
        <div class="info text-success italic">
          Zeying Gong, Tianshuai Hu, Ronghe Qiu, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">ICRA 2025</span>
           <iframe src="https://ghbtns.com/github-btn.html?user=Zeying-Gong&repo=Falcon&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2409.13244">Paper</a>]
          [<a href="https://zeying-gong.github.io/projects/falcon/">Project page</a>]
          [<a href="https://github.com/Zeying-Gong/Falcon">Code</a>]
          [<a href="https://mp.weixin.qq.com/s/NT8Wk6N3S_mQwN_f39Arog">é‡å­ä½æŠ¥é“</a>]
          [<a href="https://zhuanlan.zhihu.com/p/20569173162">çŸ¥ä¹</a>]
        </div>
        <div style="clear:both"></div>
      </li>



      <li>
        <div class="imgblock"><img src="camera_ready/dicong01.png"></img></div>
        <span class="title">GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs
        </span>
        <div class="info text-success italic">
          Xinli Xu, Wenhang Ge, Dicong Qiu, ZhiFei Chen, Dongyu Yan, Zhuoyun Liu, Haoyu Zhao, Hanfeng Zhao, Shunsi Zhang, <span style="font-weight:bold">Junwei Liang*</span>, Ying-Cong Chen*
        </div>
        <div class="info"><span class="label label-info">ICCV 2025</span>
          <!--<span class="text-error">(Spotlight paper)</span>-->
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2412.11258">Paper</a>]
          [<a href="https://gaussian-property.github.io/">Project page</a>]
          [<a href="https://github.com/EnVision-Research/Gaussian-Property">Code</a>]
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/teli01.gif"></img></div>
        <span class="title">Contrastive Imitation Learning for Language-guided Multi-Task Robotic Manipulation
        </span>
        <div class="info text-success italic">
         Teli Ma, Jiaming Zhou, Zifan Wang, Ronghe Qiu, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">CoRL 2024</span>
           <iframe src="https://ghbtns.com/github-btn.html?user=TeleeMa&repo=Sigma-Agent&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/pdf/2406.09738">Paper</a>]
          [<a href="https://teleema.github.io/projects/Sigma_Agent/">Project page</a>]
          [<a href="https://github.com/TeleeMa/Sigma-Agent">Code</a>]
          [<a href="https://youtu.be/t6xLTnMFTB8">Video</a>]
          [<a href="https://zhuanlan.zhihu.com/p/4665469077">çŸ¥ä¹</a>]
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/xinyu01.png"></img></div>
        <span class="title">Prioritized Semantic Learning for Zero-shot Instance Navigation
        </span>
        <div class="info text-success italic">
         Xinyu Sun, Lizhao Liu, Hongyan Zhi, Ronghe Qiu, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">ECCV 2024</span>
          <!--<span class="text-error">(Spotlight paper)</span>-->
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/pdf/2403.11650">Paper</a>]
          [<a href="https://github.com/XinyuSun/PSL-InstanceNav">Dataset/Code/Model</a>]
          [<a href="https://zhuanlan.zhihu.com/p/762118739">çŸ¥ä¹</a>]
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/dragtraffic.png"></img></div>
        <span class="title">Dragtraffic: Interactive and Controllable Traffic Scene Generation for Autonomous Driving
        </span>
        <div class="info text-success italic">
        Sheng WANG, Ge SUN, Fulong MA, Tianshuai HU, Qiang QIN, Yongkang SONG, Lei ZHU, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">IROS 2024</span>
          <!--<span class="text-error">(Spotlight paper)</span>-->
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/pdf/2404.12624.pdf">Paper</a>]
          [<a href="https://chantsss.github.io/Dragtraffic/">Project page</a>]
          [<a href="https://github.com/chantsss/Dragtraffic">Code</a>]
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/xiaoyu01.png"></img></div>
        <span class="title">Open-Vocabulary 3D Semantic Segmentation with Text-to-Image Diffusion Models
        </span>
        <div class="info text-success italic">
         Xiaoyu Zhu, Hao Zhou, Pengfei Xing, Long Zhao, Hao Xu, <span style="font-weight:bold">Junwei Liang</span>, Alexander Hauptmann, Ting Liu, Andrew Gallagher
        </div>
        <div class="info"><span class="label label-info">ECCV 2024</span>
          <!--<span class="text-error">(Spotlight paper)</span>-->
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/pdf/2407.13642.pdf">Paper</a>]
          [<a href="https://diff2scene.github.io/">Project Page</a>]
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/naacl24.png"></img></div>
        <span class="title">An Examination of the Compositionality of Large Generative Vision-Language Models
        </span>
        <div class="info text-success italic">
          Teli Ma, Rong Li, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">NAACL 2024</span>
          <!--<span class="text-error">(Spotlight paper)</span>-->
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2308.10509">Paper</a>]
          [<a href="https://teleema.github.io/projects/SADE/sade.html">Project Page</a>]
          [<a href="https://github.com/TeleeMa/SADE">Dataset</a>]
          [<a href="https://zhuanlan.zhihu.com/p/703924239">çŸ¥ä¹</a>]
        </div>
        <div style="clear:both"></div>
      </li>

      <hr/>

      Preprint.







      <li>
        <div class="imgblock"><img src="camera_ready/dicong01.gif"></img></div>
        <span class="title">Open-vocabulary Mobile Manipulation in Unseen Dynamic Environments with 3D Semantic Maps
        </span>
        <div class="info text-success italic">
         Dicong Qiu, Wenzong Ma, Zhenfu Pan, Hui Xiong, <span style="font-weight:bold">Junwei Liang*</span>
        </div>

        <div class="stuff">
          [<a href="https://arxiv.org/pdf/2406.18115">Paper</a>]
          [<a href="https://davidqiu1993.github.io/3dsmaps/">Project page</a>]
          [<a href="https://youtu.be/xE6M6WKw-0k">Video</a>]
        </div>
        <div style="clear:both"></div>
      </li>

      <!--
      <li>
        <div class="imgblock"><img src="camera_ready/jian01.png"></img></div>
        <span class="title">Vision-Language Models Meet Meteorology: Developing Models for Extreme Weather Events Detection with Heatmaps
        </span>
        <div class="info text-success italic">
          Jian Chen, Peilin Zhou, Yining Hua, Dading Chong, Meng Cao, Yaowei Li, Zixuan Yuan, Bing Zhu, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">ArXiv 2024</span>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/pdf/2406.09838">Paper</a>]
          [<a href="https://github.com/AlexJJJChen/Climate-Zoo">Code/Data</a>]
        </div>
        <div style="clear:both"></div>
      </li>
      -->


      <!-- old pub
      <li>
        <div class="imgblock"><img src="camera_ready/acl24.png"></img></div>
        <span class="title">FinTextQA: A Dataset for Long-form Financial Question Answering
        </span>
        <div class="info text-success italic">
          Jian Chen, Peilin Zhou, Yining Hua, Yingxin Loh, Kehui Chen, Ziyuan Li, Bing Zhu*, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">ACL 2024</span>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2405.09980">Paper</a>]
          [<a href="https://github.com/AlexJJJChen/FinTextQA">Code/Model</a>]
        </div>
        <div style="clear:both"></div>
      </li>


      <li>
        <div class="imgblock"><img src="camera_ready/cvpr24precog.png"></img></div>
        <span class="title">VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate Spatiotemporal Forecasting
        </span>
        <div class="info text-success italic">
          Yujin Tang, Peijie Dong, Zhenheng Tang, Xiaowen Chu, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">CVPR 2024 Precognition Workshop</span>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2403.16536">Paper</a>]
          [<a href="https://github.com/yyyujintang/VMRNN-PyTorch">Project Page/Code/Model</a>]
          <iframe src="https://ghbtns.com/github-btn.html?user=yyyujintang&repo=VMRNN-PyTorch&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/ijcai24.png"></img></div>
        <span class="title">PatchMixer: A Patch-Mixing Architecture for Long-Term Time Series Forecasting
        </span>
        <div class="info text-success italic">
          Zeying Gong, Yujin Tang, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">IJCAI 2024 Workshop: DATA SCIENCE MEETS OPTIMISATION</span>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2310.00655">Paper</a>]
          [<a href="https://github.com/Zeying-Gong/PatchMixer">Project Page/Code/Model</a>]
          <iframe src="https://ghbtns.com/github-btn.html?user=Zeying-Gong&repo=PatchMixer&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/postrainbench.png"></img></div>
        <span class="title">PostRainBench: A comprehensive benchmark and a new model for precipitation forecasting
        </span>
        <div class="info text-success italic">
          Yujin Tang, Jiaming Zhou, Xiang Pan, Zeying Gong, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">ICLR 2024 Workshop: Tackling Climate Change with Machine Learning</span>
          <span class="text-error">(Spotlight paper)</span>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2310.02676">Paper</a>]
          [<a href="https://github.com/yyyujintang/PostRainBench">Project Page/Code/Model</a>]
          <iframe src="https://ghbtns.com/github-btn.html?user=yyyujintang&repo=PostRainBench&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/cvpr24wad.png"></img></div>
        <span class="title">TFNet: Exploiting Temporal Cues for Fast and Accurate LiDAR Semantic Segmentation
        </span>
        <div class="info text-success italic">
          Rong Li, ShiJie Li, Xieyuanli Chen, Teli Ma, Wang Hao, Juergen Gall, <span style="font-weight:bold">Junwei Liang*</span>
        </div>
        <div class="info"><span class="label label-info">CVPR 2024 Workshop on Autonomous Driving</span>
        </div>
        <div class="stuff">
          [<a href="https://openaccess.thecvf.com/content/CVPR2024W/WAD/papers/Li_TFNet_Exploiting_Temporal_Cues_for_Fast_and_Accurate_LiDAR_Semantic_CVPRW_2024_paper.pdf">Paper</a>]
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/cvpr23.png"></img></div>
        <span class="title">STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition
        </span>
        <div class="info text-success italic">
          Xiaoyu Zhu, Po-Yao Huang, <span style="font-weight:bold">Junwei Liang</span>, Celso M de Melo, Alexander G Hauptmann
        <div class="info"><span class="label label-info">CVPR 2023</span>
        </div>
        <div class="stuff">
          [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_STMT_A_Spatial-Temporal_Mesh_Transformer_for_MoCap-Based_Action_Recognition_CVPR_2023_paper.pdf">Paper</a>]
          [<a href="https://github.com/zgzxy001/STMT">Project Page/Code/Model</a>]
          <iframe src="https://ghbtns.com/github-btn.html?user=zgzxy001&repo=STMT&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div style="clear:both"></div>
      </li>
      -->

      <!-- Publication from before -->
      <!--
      <li>
        <div class="imgblock"><img src="camera_ready/cross_action.jpg"></img></div>
        <span class="title">Multi-dataset Training of Transformers for Robust Action Recognition
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Enwei Zhang, Jun Zhang, Chunhua Shen</div>
        <div class="info"><span class="label label-info">NeurIPS 2022</span>
          <span class="text-error">(<a href="https://nips.cc/virtual/2022/spotlight/65262">Spotlight paper</a>, 3.7% acceptance rate, 384/10411)</span>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2209.12362">Paper</a>]
          [<a href="https://github.com/JunweiLiang/MultiTrain">Project Page/Code/Model</a>]
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=MultiTrain&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/multiverse.gif"></img></div>
        <span class="title">The Garden of Forking Paths: Towards Multi-Future Trajectory Prediction
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Lu Jiang, Kevin Murphy, Ting Yu, Alexander Hauptmann</div>
        <div class="info"><span class="label label-info">CVPR 2020</span> &nbsp;
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=Multiverse&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/1912.06445" target="_blank">[Paper]</a>
          <a class="" href="https://precognition.team/next/multiverse/resources/cvpr2020.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://youtu.be/RW45YQHxIhk" target="_blank">[Demo Video]</a>
          <a class="" href="https://precognition.team/next/multiverse" target="_blank">[Project Page/Code/Model]</a>
          <a href="https://medium.com/@junweil/cvpr20-the-garden-of-forking-paths-towards-multi-future-trajectory-prediction-df23221dc9f8">[blog]</a>
          <a href="https://zhuanlan.zhihu.com/p/148343447">[çŸ¥ä¹]</a>
          <a href="https://research.google/pubs/pub49224/">[Google Research]</a>
          <a href="https://mp.weixin.qq.com/s/s6bk5psLwpGpO1VwtQqo_g">[è¯»èŠ¯æœ¯å­¦æœ¯æŠ¥å‘Š]</a>
          <a href="https://sites.google.com/di.uniroma1.it/patcast/program?authuser=0">[Invited presentation at ICPR'20 pattern forecasting workshop]</a>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock" style="height:280px"><img src="camera_ready/next.gif"></img></div>
        <span class="title">Peeking into the Future: Predicting Future Person Activities and Locations in Videos
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Lu Jiang, Juan Carlos Niebles, Alexander Hauptmann, Li Fei-Fei</div>
        <div class="info"><span class="label label-info">CVPR 2019</span> <span class="text-error">(Translated and reported by multiple Chinese media (<a href="https://weixin.sogou.com/weixin?type=1&s_from=input&query=%E9%87%8F%E5%AD%90%E4%BD%8D" target="_blank">é‡å­ä½</a> & <a href="https://weixin.sogou.com/weixin?type=1&s_from=input&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83" target="_blank">æœºå™¨ä¹‹å¿ƒ</a>, 02/13/2019), with 30k+ views in a week.)</span> </div>
        <div class="info"><span class="text-error">#1 Tensorflow-based code on <a href="https://paperswithcode.com/task/trajectory-prediction">PaperWithCode</a> in Trajectory Prediction task. </span> <iframe src="https://ghbtns.com/github-btn.html?user=google&repo=next-prediction&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/1902.03748" target="_blank">[Paper]</a>
          <a class="" href="camera_ready/future19.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://www.youtube.com/watch?v=NyrGxGoS01U" target="_blank">[Demo Video]</a>
          <a class="" href="https://precognition.team/next" target="_blank">[Project Page/Code/Model]</a>
          <a href="https://research.google/pubs/pub47873/">[Google Research]</a>
        </div>
        <div style="clear:both"></div>
      </li>
      -->


    </ol>
  </div>



</div>


<!--
	a Junwei Liang's production
	contact: junweiliang1114@gmail.com
-->
</body>
</html>
