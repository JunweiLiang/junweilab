<html>
<head>
	<meta  http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=1" />
	<link rel="stylesheet" type="text/css" href="utils/bootstrap.min.css"/>
  <script language="javascript" src="utils/jquery.min.js"></script>
	<script language="javascript" src="utils/bootstrap.min.js"></script>
  <!-- -->
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css"/>
	<link rel="stylesheet" type="text/css" href="utils/cssReset.css"/>
  <link rel="stylesheet" type="text/css" href="utils/css_layout.css"/>
	<title>Precognition Lab at HKUST (Guangzhou)</title>
  <!-- 搜索引擎优化stuff -->
	<meta name="description"
    content="Academic website for Prof. Junwei Liang's lab, the Precognition Lab. Dr. Junwei Liang is currently a tenure-track Assistant Professor at The Hong Kong University of Science and Technology (Guangzhou). He is also an affiliate assistant professor at HKUST computer science & engineering department. He was a senior researcher at Tencent Youtu Lab working on cutting-edge computer vision research and applications. Prior to that, he received his Ph.D. degree from Carnegie Mellon University, working with Prof. Alexander Hauptmann. He is the recipient of Baidu Scholarship and Yahoo Fellowship, and awarded Rising Star Award at the World AI Conference in 2020. He is the winner of several public safety video analysis competitions, including ASAPS and TRECVID ActEV. His work has helped and been reported by major news agencies like the Washington Post and New York Times. His research interests include human trajectory forecasting, action recognition, and large-scale computer vision and video analytics in general. His mission: develop AI technologies for social good.">
	<meta name="keywords" content="Junwei Liang,CMU,HKUST,HKUST-GZ,Professor,computer vision,PhD,梁俊卫,Carnegie Mellon University,The Hong Kong University of Science and Technology">

	<!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156016426-1"></script>
  <script>
  	// for Google Analytics, for free!
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-156016426-1');

  </script>

</head>
<body>
<div id="sidebar">
  <img class='me' src="resources/me.jpeg"></img>
  <br/>
  <div class="info">
    <h2 class="name">Lab Director:</h2>
    <h2 class="name">Prof. Junwei Liang</h2>
    <h2 class="name_chinese">梁俊卫</h2>
    <h2 class="email">HKUST (Guangzhou) / HKUST</h2>
    <h2 class="email">Office: E4-304</h2>
    <h2 class="link">
      <a style="font-size: 14px; color: yellow; font-weight: bold;" href="https://junweiliang.me">Personal Page</a>
    </h2>
  <!--
    <h2 class="link">
      <a href="https://www.semanticscholar.org/author/Junwei-Liang/1915796">[Semantic Scholar]</a>
      <a href="https://www.researchgate.net/profile/Junwei_Liang3">[Research Gate]</a>
    </h2>
    <h2 class="link">
      <a href="https://github.com/JunweiLiang">[Github]</a>
      <a href="https://www.linkedin.com/in/junweiliang/">[LinkedIn]</a>
      <a href="https://www.youtube.com/channel/UC-z7ZWp8Rbu2xhxnbAL_bRQ">[Youtube]</a>
    </h2>
    <h2 class="link">
      <a href="https://www.zhihu.com/people/junwei-liang-50">[知乎]</a>
      <a href="https://www.xiaohongshu.com/user/profile/62c3a783000000001b02b099">[小红书]</a>
      <a href="https://twitter.com/JunweilLiang">[Twitter]</a>
    </h2>
  -->

  </div>
  <div id="navigation">
    <a class="nav_item" href="./index.html">
      <i class="icon icon-home icon-white"></i> &nbsp; About
    </a>
    <a class="nav_item" href="./index.html#publications">
      <i class="icon icon-th-large icon-white icon-list"></i> &nbsp; Publications
    </a>
    <a class="nav_item" href="./projects.html#projects">
      <i class="icon icon-th-large icon-white"></i> &nbsp; Projects
    </a>
    <a class="nav_item" href="./index.html#members">
      <i class="icon icon-white icon-user"></i> &nbsp; Lab Members
    </a>
    <a class="nav_item" href="./internal/index.html">
      <i class="icon icon-book icon-white"></i> &nbsp; Internal Docs
    </a>

  </div>
</div>

<div id="main">
  <div class="title" style="padding:40px 0">
    <a class="title_link" id="bio" href="#bio">Precognition Lab @ HKUST (Guangzhou)</a>
    <!--<img src="https://vbr.wocr.tk/badge?page_id=JunweiLiang.precognition&right_color=green" alt=""/> -->
    <img src="https://vbr.wocr.tk/badge?page_id=JunweiLiang.JunweiLiang&right_color=green" alt=""/>
    <img id="logo" src="resources/hkustgz-logo.jpg"></img>
  </div>


  <div class="content">
    Our research lab, the Precognition Lab (智能感知与预测实验室), is interested in building human-level AI systems that could effectively perceive, reason and interact with the real world for the good of humans.
    Here is an up-to-date <a href="./projects.html#roadmap">research roadmap</a>.

    <div class="linebreak"></div>

    Our lab's computing resources include 32 RTX 3090/4090 GPUs and a cluster of 24 A6000 GPUs with a 100TB NAS. See <a href="https://www.linkedin.com/feed/update/urn:li:activity:7099940517966200832/">this post</a>.
    And we have three mobile platforms with a robot arm:

    <div class="linebreak"></div>

    <img src="resources/robot1.png" style="height:300px;margin:20px 30px 20px 0"></img>
    <img src="resources/robot2.png" style="height:300px;margin:20px 30px 20px 0"></img>
    <img src="resources/robot3.png" style="height:300px;margin:20px 30px 20px 0"></img>
    <br/>

    Check out our lab's <span style="font-weight: bold;"><a href="./projects.html#demos">cool demos</a></span>.
    <!-- <a href="https://mp.weixin.qq.com/s/otlRTlOEET3ldJ3URqw8AQ">@Jacobi.ai</a>-->

    <br/>

    Our lab has over 10K followers on social media：
    [<a href="https://www.zhihu.com/people/Iris0329">Rong's 知乎</a>]
    [<a href="https://www.zhihu.com/people/30-76-66-6">Yujin's 知乎</a>]
    [<a href="https://www.zhihu.com/people/junwei-liang-50">Junwei's 知乎</a>]
    [<a href="https://www.xiaohongshu.com/user/profile/62c3a783000000001b02b099">Junwei's 小红书</a>]
    [<a href="https://www.linkedin.com/in/junweiliang/">Junwei's LinkedIn</a>]

  </div>



  <div class="title">
    <a class="title_link" id="media" href="#media">Media Coverage</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="label label-success">12/2023</span> 雅可比机器人获得2023新一代人工智能（深圳）创业大赛三等奖
        [by <a href="https://m.163.com/tech/article/IL7I2EOJ00097U7R.html">163 news</a>, <a href="https://mp.weixin.qq.com/s/x9BYlCXb7BTQvJcZMYFnzQ">雅可比机器人</a>]
      </li>
      <li>
        <span class="label label-success">10/2023</span> Patch才是时序预测的王道？
        [by <a href="https://mp.weixin.qq.com/s/JBO7_xyMwsATO_zi4LWbzQ">圆圆的算法笔记</a>, <a href="https://mp.weixin.qq.com/s/ElRBgAWTveItXJSoZe2DLQ">kaggle竞赛宝典</a>]
        [<a href="https://browse.arxiv.org/pdf/2310.00655v1.pdf">Paper</a>]
      </li>
      <li>
        <span class="label label-success">10/2023</span> TFNet：利用时间线索实现快速且精确的激光雷达语义分割
        [by <a href="https://mp.weixin.qq.com/s/V5uuTEwC8gFepIdXzFYAxA">自动驾驶专栏</a>]
        [<a href="https://arxiv.org/pdf/2309.07849.pdf">Paper</a>]
      </li>
      <li>
        <span class="label label-success">06/2023</span> Honorable Mention at the International Robot Manipulation Competition, Robothon (2023机器人马拉松挑战赛)
        [by <a href="https://mp.weixin.qq.com/s/V5uuTEwC8gFepIdXzFYAxA">香港科技大学（广州）公众号</a>]
        [<a href="https://automatica-munich.com/en/munich-i/robothon/teams/#team_submissions">Robothon 2023</a>]
      </li>
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="news" href="#news">News</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="label label-info">10/2023</span> Co-organizing the Open-world Visual Perception Workshop (“开放世界下的视觉感知和增强”主题论坛) @PRCV 2023
        [<a href="https://mp.weixin.qq.com/s/ib9aKBhQhoaAFqZB93F3wQ">Schedule</a>]
      </li>
      <li>
        <!--<a href="https://www.linkedin.com/posts/khoa-luu-90900215_cvpr2023-activity-7009293998955655168-M4lu?utm_source=share&utm_medium=member_desktop">Precognition workshop</a>-->
        <span class="label label-info">01/2023</span> I am co-organizing the <a href="https://sites.google.com/view/ieeecvf-cvpr2023-precognition">The 5th workshop on Precognition: Seeing through the Future</a> @CVPR 2023.
        [<a href="https://www.linkedin.com/posts/junweiliang_cvpr2023-workshop-computervision-activity-7030466054787121152-NacF?utm_source=share&utm_medium=member_desktop">Call For Papers</a>] [<a href="https://zhuanlan.zhihu.com/p/603134088">知乎</a>]
      </li>
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="members" href="#members">Lab Members</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <a href="https://teleema.github.io/">Teli Ma</a>
        [<a href="https://github.com/TeleeMa">Github</a>]
        [<a href="https://scholar.google.com/citations?user=arny77IAAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="https://scholar.google.com/citations?user=b3y40w8AAAAJ">Jiaming Zhou</a>
        [<a href="https://scholar.google.com/citations?user=b3y40w8AAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="http://www.davidqiu.com/">Dicong Qiu</a>
        [<a href="https://scholar.google.com/citations?user=ZFmSow8AAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="https://rongli.tech/">Rong Li</a>
        [<a href="https://www.zhihu.com/people/Iris0329">知乎</a>]
        [<a href="https://github.com/iris0329">Github</a>]
        [<a href="https://scholar.google.com/citations?user=M68wBgkAAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="https://github.com/yyyujintang">Yujin Tang</a>
        [<a href="https://www.zhihu.com/people/30-76-66-6">知乎</a>]
        [<a href="https://github.com/yyyujintang">Github</a>]
      </li>

      <li>
        <a href="https://github.com/ConnerQiu">Ronghe Qiu</a>
        [<a href="https://github.com/ConnerQiu">Github</a>]
      </li>

      <li>
        <a href="https://jhuiye.com/">Jinhui Ye</a>
        [<a href="https://github.com/JinhuiYE">Github</a>]
        [<a href="https://scholar.google.com/citations?user=RpXhJu0AAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="https://zeying-gong.github.io/">Zeying Gong</a>
        [<a href="https://github.com/Zeying-Gong">Github</a>]
        [<a href="https://scholar.google.com/citations?user=ze2Wh9EAAAAJ">Google Scholar</a>]
      </li>

      <li>
        Jian Chen
      </li>

      <li>
        <a href="https://github.com/XinyuSun">Xinyu Sun</a>
        [<a href="https://github.com/XinyuSun">Github</a>]
        [<a href="https://scholar.google.com/citations?user=ALq8sMgAAAAJ">Google Scholar</a>]
      </li>
      <li>
        <a href="https://zgzxy001.github.io/">Xiaoyu Zhu</a> (Ph.D. student @CMU, co-advising)
        [<a href="https://github.com/zgzxy001">Github</a>]
        [<a href="https://twitter.com/XiaoyuZhu10">Twitter</a>]
        [<a href="https://scholar.google.com/citations?user=T4Dc5zEAAAAJ&hl=en">Google Scholar</a>]
      </li>

    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="publications" href="#publications">Publications</a>
  </div>

  <div class="content publications">
    <ol>
      <li>
        <div class="imgblock"><img src="camera_ready/multiverse.gif"></img></div>
        <span class="title">The Garden of Forking Paths: Towards Multi-Future Trajectory Prediction
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Lu Jiang, Kevin Murphy, Ting Yu, Alexander Hauptmann</div>
        <div class="info"><span class="label label-info">CVPR 2020</span> &nbsp;
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=Multiverse&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/1912.06445" target="_blank">[Paper]</a>
          <a class="" href="https://precognition.team/next/multiverse/resources/cvpr2020.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://youtu.be/RW45YQHxIhk" target="_blank">[Demo Video]</a>
          <a class="" href="https://precognition.team/next/multiverse" target="_blank">[Project Page/Code/Model]</a>
          <a href="https://medium.com/@junweil/cvpr20-the-garden-of-forking-paths-towards-multi-future-trajectory-prediction-df23221dc9f8">[blog]</a>
          <a href="https://zhuanlan.zhihu.com/p/148343447">[知乎]</a>
          <a href="https://research.google/pubs/pub49224/">[Google Research]</a>
          <a href="https://mp.weixin.qq.com/s/s6bk5psLwpGpO1VwtQqo_g">[读芯术学术报告]</a>
          <a href="https://sites.google.com/di.uniroma1.it/patcast/program?authuser=0">[Invited presentation at ICPR'20 pattern forecasting workshop]</a>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <!--<div class="imgblock"><img src="camera_ready/peekfuture.png"></img></div>-->
        <div class="imgblock" style="height:280px"><img src="camera_ready/next.gif"></img></div>
        <span class="title">Peeking into the Future: Predicting Future Person Activities and Locations in Videos
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Lu Jiang, Juan Carlos Niebles, Alexander Hauptmann, Li Fei-Fei</div>
        <div class="info"><span class="label label-info">CVPR 2019</span> <span class="text-error">(Translated and reported by multiple Chinese media (<a href="https://weixin.sogou.com/weixin?type=1&s_from=input&query=%E9%87%8F%E5%AD%90%E4%BD%8D" target="_blank">量子位</a> & <a href="https://weixin.sogou.com/weixin?type=1&s_from=input&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83" target="_blank">机器之心</a>, 02/13/2019), with 30k+ views in a week.)</span> </div>
        <div class="info"><span class="text-error">#1 Tensorflow-based code on <a href="https://paperswithcode.com/task/trajectory-prediction">PaperWithCode</a> in Trajectory Prediction task. </span> <iframe src="https://ghbtns.com/github-btn.html?user=google&repo=next-prediction&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/1902.03748" target="_blank">[Paper]</a>
          <a class="" href="camera_ready/future19.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://www.youtube.com/watch?v=NyrGxGoS01U" target="_blank">[Demo Video]</a>
          <a class="" href="https://precognition.team/next" target="_blank">[Project Page/Code/Model]</a>
          <a href="https://research.google/pubs/pub47873/">[Google Research]</a>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/cross_action.jpg"></img></div>
        <span class="title">Multi-dataset Training of Transformers for Robust Action Recognition
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Enwei Zhang, Jun Zhang, Chunhua Shen</div>
        <div class="info"><span class="label label-info">NeurIPS 2022</span>
          <span class="text-error">(<a href="https://nips.cc/virtual/2022/spotlight/65262">Spotlight paper</a>, 3.7% acceptance rate, 384/10411)</span>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2209.12362">Paper</a>]
          [<a href="https://github.com/JunweiLiang/MultiTrain">Project Page/Code/Model</a>]
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=MultiTrain&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div style="clear:both"></div>
      </li>


    </ol>
  </div>



</div>


<!--
	a Junwei Liang's production
	contact: junweiliang1114@gmail.com
-->
</body>
</html>
