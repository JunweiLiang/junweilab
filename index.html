<html>
<head>
	<meta  http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=1" />
	<link rel="stylesheet" type="text/css" href="utils/bootstrap.min.css"/>
  <script language="javascript" src="utils/jquery.min.js"></script>
	<script language="javascript" src="utils/bootstrap.min.js"></script>
  <!-- -->
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css"/>
	<link rel="stylesheet" type="text/css" href="utils/cssReset.css"/>
  <link rel="stylesheet" type="text/css" href="utils/css_layout.css"/>
	<title>Precognition Lab at HKUST (Guangzhou)</title>
  <!-- 搜索引擎优化stuff -->
	<meta name="description"
    content="Academic website for Prof. Junwei Liang's lab, the Precognition Lab. Dr. Junwei Liang is currently a tenure-track Assistant Professor at The Hong Kong University of Science and Technology (Guangzhou). He is also an affiliate assistant professor at HKUST computer science & engineering department. He was a senior researcher at Tencent Youtu Lab working on cutting-edge computer vision research and applications. Prior to that, he received his Ph.D. degree from Carnegie Mellon University, working with Prof. Alexander Hauptmann. He is the recipient of Baidu Scholarship and Yahoo Fellowship, and awarded Rising Star Award at the World AI Conference in 2020. He is the winner of several public safety video analysis competitions, including ASAPS and TRECVID ActEV. His work has helped and been reported by major news agencies like the Washington Post and New York Times. His research interests include human trajectory forecasting, action recognition, and large-scale computer vision and video analytics in general. His mission: develop AI technologies for social good.">
	<meta name="keywords" content="Junwei Liang,CMU,HKUST,HKUST-GZ,Professor,computer vision,PhD,梁俊卫,Carnegie Mellon University,The Hong Kong University of Science and Technology">

	<!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156016426-1"></script>
  <script>
  	// for Google Analytics, for free!
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-156016426-1');

  </script>

</head>
<body>
<div id="sidebar">
  <img class='me' src="resources/me.jpeg"></img>
  <br/>
  <div class="info">
    <h2 class="name">Lab Director:</h2>
    <h2 class="name">Prof. Junwei Liang</h2>
    <h2 class="name_chinese">梁俊卫</h2>
    <h2 class="email">HKUST (Guangzhou) / HKUST</h2>
    <h2 class="email">Office: E4-304</h2>
    <h2 class="link">
      <a style="font-size: 14px; color: yellow; font-weight: bold;" href="https://junweiliang.me">Personal Page</a>
    </h2>
  <!--
    <h2 class="link">
      <a href="https://www.semanticscholar.org/author/Junwei-Liang/1915796">[Semantic Scholar]</a>
      <a href="https://www.researchgate.net/profile/Junwei_Liang3">[Research Gate]</a>
    </h2>
    <h2 class="link">
      <a href="https://github.com/JunweiLiang">[Github]</a>
      <a href="https://www.linkedin.com/in/junweiliang/">[LinkedIn]</a>
      <a href="https://www.youtube.com/channel/UC-z7ZWp8Rbu2xhxnbAL_bRQ">[Youtube]</a>
    </h2>
    <h2 class="link">
      <a href="https://www.zhihu.com/people/junwei-liang-50">[知乎]</a>
      <a href="https://www.xiaohongshu.com/user/profile/62c3a783000000001b02b099">[小红书]</a>
      <a href="https://twitter.com/JunweilLiang">[Twitter]</a>
    </h2>
  -->

  </div>
  <div id="navigation">
    <a class="nav_item" href="./index.html">
      <i class="icon icon-home icon-white"></i> &nbsp; About
    </a>
    <a class="nav_item" href="./projects.html#projects">
      <i class="icon icon-th-large icon-white"></i> &nbsp; Projects
    </a>
    <a class="nav_item" href="./index.html#members">
      <i class="icon icon-white icon-user"></i> &nbsp; Lab Members
    </a>
    <a class="nav_item" href="./internal/index.html">
      <i class="icon icon-book icon-white"></i> &nbsp; Internal Docs
    </a>

  </div>
</div>

<div id="main">
  <div class="title" style="padding:40px 0">
    <a class="title_link" id="bio" href="#bio">The Precognition Lab</a>
    <!--<img src="https://vbr.wocr.tk/badge?page_id=JunweiLiang.precognition&right_color=green" alt=""/> -->
    <img src="https://vbr.wocr.tk/badge?page_id=JunweiLiang.JunweiLiang&right_color=green" alt=""/>
    <img id="logo" src="resources/hkustgz-logo.jpg"></img>
  </div>


  <div class="content">
    Our research lab, the Precognition Lab (智能感知与预测实验室), is interested in building human-level AI systems that could effectively perceive, reason and interact with the real-world for the good of humans.
    Here is a up-to-date research roadmap: <a href="https://docs.google.com/presentation/d/1C1raWd6a0Df7ZOK8dj4ZQLld2YQ0xqDf3h2vSgLnJZg/edit?usp=sharing">[中文版]</a> [English version].

    <div class="linebreak"></div>

    Our lab's computing resources include 32 RTX 3090/4090 GPUs and a cluster of 24 A6000 GPUs with a 100TB NAS. See <a href="https://www.linkedin.com/feed/update/urn:li:activity:7099940517966200832/">this post</a>.
    And we have two mobile platforms with a robot arm:

    <div class="linebreak"></div>

    <img src="resources/robot1.png" style="height:300px;margin:20px 30px 20px 0"></img>
    <img src="resources/robot2.png" style="height:300px;margin:20px 30px 20px 0"></img>
    <br/>

    Check out our Embodied AI demo <a href="https://mp.weixin.qq.com/s/otlRTlOEET3ldJ3URqw8AQ">@Jacobi.ai</a>

    <br/>

    我们是拥有上万粉丝/followers的实验室：
    [<a href="https://www.zhihu.com/people/Iris0329">Rong's 知乎</a>]
    [<a href="https://www.zhihu.com/people/30-76-66-6">Yujin's 知乎</a>]
    [<a href="https://www.zhihu.com/people/junwei-liang-50">Junwei's 知乎</a>]
    [<a href="https://www.linkedin.com/in/junweiliang/">Junwei's LinkedIn</a>]

  </div>

  <div class="title">
    <a class="title_link" id="members" href="#members">Lab Members</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <a href="https://teleema.github.io/">Teli Ma</a>
        [<a href="https://github.com/TeleeMa">Github</a>]
        [<a href="https://scholar.google.com/citations?user=arny77IAAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="https://scholar.google.com/citations?user=b3y40w8AAAAJ">Jiaming Zhou</a>
        [<a href="https://scholar.google.com/citations?user=b3y40w8AAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="http://www.davidqiu.com/">Dicong Qiu</a>
        [<a href="https://scholar.google.com/citations?user=ZFmSow8AAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="https://rongli.tech/">Rong Li</a>
        [<a href="https://www.zhihu.com/people/Iris0329">知乎</a>]
        [<a href="https://github.com/iris0329">Github</a>]
        [<a href="https://scholar.google.com/citations?user=M68wBgkAAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="https://github.com/yyyujintang">Yujin Tang</a>
        [<a href="https://www.zhihu.com/people/30-76-66-6">知乎</a>]
        [<a href="https://github.com/yyyujintang">Github</a>]
      </li>

      <li>
        <a href="https://github.com/ConnerQiu">Ronghe Qiu</a>
        [<a href="https://github.com/ConnerQiu">Github</a>]
      </li>

      <li>
        <a href="https://jhuiye.com/">Jinhui Ye</a>
        [<a href="https://github.com/JinhuiYE">Github</a>]
        [<a href="https://scholar.google.com/citations?user=RpXhJu0AAAAJ">Google Scholar</a>]
      </li>

      <li>
        <a href="https://zeying-gong.github.io/">Zeying Gong</a>
        [<a href="https://github.com/Zeying-Gong">Github</a>]
        [<a href="https://scholar.google.com/citations?user=ze2Wh9EAAAAJ">Google Scholar</a>]
      </li>

      <li>
        Jian Chen
      </li>

      <li>
        <a href="https://github.com/XinyuSun">Xinyu Sun</a>
        [<a href="https://github.com/XinyuSun">Github</a>]
        [<a href="https://scholar.google.com/citations?user=ALq8sMgAAAAJ">Google Scholar</a>]
      </li>
      <li>
        <a href="https://zgzxy001.github.io/">Xiaoyu Zhu</a> (Ph.D. student @CMU, co-advising)
        [<a href="https://github.com/zgzxy001">Github</a>]
        [<a href="https://twitter.com/XiaoyuZhu10">Twitter</a>]
        [<a href="https://scholar.google.com/citations?user=T4Dc5zEAAAAJ&hl=en">Google Scholar</a>]
      </li>

    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="news" href="#news">News</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="label label-info">10/2023</span> Co-organizing the Open-world Visual Perception Workshop (“开放世界下的视觉感知和增强”主题论坛) @PRCV 2023
        [<a href="https://mp.weixin.qq.com/s/ib9aKBhQhoaAFqZB93F3wQ">Schedule</a>]
      </li>
      <li>
        <!--<a href="https://www.linkedin.com/posts/khoa-luu-90900215_cvpr2023-activity-7009293998955655168-M4lu?utm_source=share&utm_medium=member_desktop">Precognition workshop</a>-->
        <span class="label label-info">01/2023</span> I am co-organizing the <a href="https://sites.google.com/view/ieeecvf-cvpr2023-precognition">The 5th workshop on Precognition: Seeing through the Future</a> @CVPR 2023.
        [<a href="https://www.linkedin.com/posts/junweiliang_cvpr2023-workshop-computervision-activity-7030466054787121152-NacF?utm_source=share&utm_medium=member_desktop">Call For Papers</a>] [<a href="https://zhuanlan.zhihu.com/p/603134088">知乎</a>]
      </li>
      <li>
        <span class="label label-info">10/2022</span> <span style="font-weight: bold">Two</span> papers accepted at <span style="font-weight: bold">NeurIPS 2022</span>.
        [<a href="https://arxiv.org/abs/2209.12362">Multi-Action</a> (<a href="https://nips.cc/virtual/2022/spotlight/65262" style="color:red">Spotlight paper</a>, 3.7% acceptance rate, 384/10411)]
        [<a href="https://arxiv.org/abs/2209.13307">Video Retrieval</a>]
      </li>
      <li>
        <span class="label label-info">06/2022</span> Achieved <span style="font-weight:bold;">second-place</span> out of 150 teams on the <a href="https://arxiv.org/pdf/2204.10380.pdf">public leaderboard</a> of the Naturalist Driver Action Recognition Task - AI City Challenge @ CVPR 2022.
        [<a href="https://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Liang_Stargazer_A_Transformer-Based_Driver_Action_Detection_System_for_Intelligent_Transportation_CVPRW_2022_paper.pdf">CVPRW Paper</a>]
        [<a href="https://www.youtube.com/watch?v=u4CrNKt4P54">Presentation</a>] [<a href="https://github.com/JunweiLiang/aicity_action">Code and Model</a>]
      </li>
      <li>
        <span class="label label-info">10/2021</span> Published a <a href="https://www.techbeat.net/talk-info?id=588">research talk</a> at TechBeat.net on Pedestrian Trajectory Prediction. [<a href="https://www.techbeat.net/talk-info?id=588">将门TechBeat</a>] [<a href="https://www.bilibili.com/video/BV1Y44y1x7nv/">B站</a>]
      </li>
      <li>
        <span class="label label-info">08/2021</span> 1 paper accepted by <span style="font-weight:bold">ICCV 2021</span>.
      </li>
      <li>
        <span class="label label-info">08/2021</span> Our <a href="https://vera.cs.cmu.edu">VERA</a> system helps another major <span style="font-weight:bold">Washington Post</span> news report. [<a href="https://www.washingtonpost.com/world/interactive/2021/myanmar-crackdown-military-coup/">link</a>]
          <a href="https://www.washingtonpost.com/world/interactive/2021/myanmar-crackdown-military-coup/">
            <img class="press" src="resources/wapo.png"></img>
          </a>
      </li>
      <li>
        <span class="label label-info">04/2021</span> Featured in a <a href="https://www.washingtonpost.com/investigations/interactive/2021/dc-police-records-capitol-riot/">front-page news report</a> (04/15) by Washington Post using crowding counting technologies. [<a href="https://www.youtube.com/watch?v=rsQTY9083r8?t=1086">video</a>] [<a href="https://www.zhihu.com/zvideo/1366151651770834944">知乎</a>]
        <a href="https://www.washingtonpost.com/investigations/interactive/2021/dc-police-records-capitol-riot/">
            <img class="press" src="resources/wapo.png"></img>
        </a>
      </li>
      <li>
        <span class="label label-info">01/2021</span> <span style="font-weight: bold;">Invited presentation</span> at ICPR'20 pattern forecasting workshop. [<a href="https://sites.google.com/di.uniroma1.it/patcast/program?authuser=0">link</a>]
      </li>
      <li>
        <span class="label label-info">09/2020</span> We won the <a href="https://www.nist.gov/ctl/pscr/open-innovation-prize-challenges/past-prize-challenges/2020-automated-stream-analysis">Automated Streams Analysis for Public Safety Challenge</a> with a <a href="https://www.herox.com/ASAPS1/update/3483">$30k prize</a>.
      </li>
      <li>
        <span class="label label-info">08/2020</span> Our <a href="https://arxiv.org/abs/2006.16479">paper</a> has been accepted by WACV 2021 (one strong-accept) and <span style="font-weight:bold">reported by CMU news</span>:
        <a href="https://www.cmu.edu/news/stories/archives/2020/august/drones-hurricane-damage.html">
            <img class="press" src="resources/cmu.png"></img>
        </a>
      </li>
      <li>
        <span class="label label-info">08/2020</span> Analyzed videos for journalist from <span style="font-weight:bold">the Washington Post</span> on a <a href="https://www.washingtonpost.com/sports/2020/08/26/redskins-cheerleaders-video-daniel-snyder-washington/">major news</a>.
          <a href="https://www.washingtonpost.com/sports/2020/08/26/redskins-cheerleaders-video-daniel-snyder-washington/">
            <img class="press" src="resources/wapo.png"></img>
          </a>
      </li>
      <li>
        <span class="label label-info">07/2020</span> <a href="https://next.cs.cmu.edu/simaug/"><span style="font-style: italic;">SimAug</span></a> paper accepted by <span style="font-weight:bold">ECCV 2020</span>.
      </li>
      <li>
        <span class="label label-info">06/2020</span> <a href="https://next.cs.cmu.edu/multiverse/index.html"><span style="font-style: italic;">Multiverse</span></a> (<span style="font-weight:bold">CVPR 2020</span>) code and dataset are released! [<a href="https://medium.com/@junweil/cvpr20-the-garden-of-forking-paths-towards-multi-future-trajectory-prediction-df23221dc9f8">blog</a>] [<a href="https://zhuanlan.zhihu.com/p/148343447">知乎</a>] [<a href="https://github.com/JunweiLiang/Multiverse">code</a>]
      </li>
      <li>
        <span class="label label-info">09/2019</span> Our <a href="https://vera.cs.cmu.edu/">Shooter Localization System</a> won <span style="font-weight:bold">Best Demo</span> award at <a href="https://cbmi2019.org/">CBMI2019</a>. [<a href="https://vera.cs.cmu.edu/" target="_blank">Project Site</a>]
        <br/>Press Coverage:
          <a href="https://www.cmu.edu/news/stories/archives/2019/november/system-locates-shooters-using-smartphone-video.html">
            <img class="press" src="resources/cmu.png"></img>
          </a>,
          <a href="https://pittsburgh.cbslocal.com/2019/11/20/cmu-develops-video-system-locate-mass-shooters/">
            <img class="press" src="resources/cbs.png"></img>
          </a>,
          <a href="https://www.post-gazette.com/business/tech-news/2019/11/20/Carnegie-Mellon-CMU-develops-cellphone-smartphone-video-system-location-shooter-triangulate/stories/201911200101">
            <img class="press" src="resources/post.png"></img>
          </a>,
          <a href="https://gizmodo.com/smartphone-videos-can-now-be-analyzed-and-used-to-pinpo-1839979803">
            <img class="press" src="resources/gizmodo.png"></img>
          </a>,
          <a href="https://www.dailymail.co.uk/sciencetech/article-7707501/Carnegie-Mellon-aims-end-pro-longed-massacres-locates-active-shooters.html">
            <img class="press" src="resources/dailymail.png"></img>
          </a>,
          <a href="https://www.techspot.com/news/82881-researchers-develop-system-can-pinpoint-shooter-location-using.html">
            <img class="press" src="resources/techspot.png"></img>
          </a>
      </li>
      <li>
        <span class="label label-info">06/2019</span> Presented Future Prediction paper at <span style="font-weight:bold">CVPR 2019</span>. It was reported by the media and it received <span style="font-weight:bold">30k+ views</span> in a week. <a href="https://next.cs.cmu.edu" target="_blank"><i title="Go to project page" class="icon-zoom-in"></i></a> [<a href="https://twitter.com/jcniebles/status/1141366303921303552" target="_blank">Tweets</a>]
      </li>
      <li><span class="label label-info">04/2019</span> Our CMU team's (INF & MUDSML) system achieved the <span style="font-weight:bold">best performance</span> on the <a href="https://actev.nist.gov/prizechallenge#tab_leaderboard" target="_blank">activity detection challenge</a> (<a href="resources/actev-prizechallenge-06-2019.png" target="_blank">Cached</a>) in surveillance videos hosted by NIST & IARPA. <!--The competitors include all other DIVA-funded teams from universities and companies as well as other strong participants from all over the world.--> We have released our code and model for Object Detection & Tracking <a href="https://github.com/JunweiLiang/Object_Detection_Tracking">here</a>. </li>
      <li><span class="label label-info">12/2018</span> <span style="font-weight:bold">MemexQA</span> paper accepted by <span style="font-weight:bold">TPAMI 2019</span>. <a href="https://precognition.team/memexqa" target="_blank"><i title="Go to project page" class="icon-zoom-in"></i></a></li>
    </ul>
  </div>




  <div class="title">
    <a class="title_link" id="media" href="#media">Demos / Project Sites</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span style="font-weight: bold">ChatGPT + Robotics @Jacobi.ai</span>
        [<a href="https://mp.weixin.qq.com/s/otlRTlOEET3ldJ3URqw8AQ">Demo1</a>]
        [<a href="https://mp.weixin.qq.com/s/Gx65LpDEWdwBYfbP-m2Esw">Demo2</a>]
        [<a href="https://36kr.jp/248510/">News Report</a>]
      </li>
      <li>
        <span style="font-weight: bold">Pedestrian Trajectory Prediction</span>
        [<a href="next/">Next-Prediction</a>] [<a href="next/multiverse">Multiverse</a>]
      </li>
      <li>
        <span style="font-weight: bold">Efficient Action Detection</span>
      </li>
      <li>
        <span style="font-weight: bold">Zero-shot Video Retrieval</span>
      </li>
      <li>
        <span style="font-weight: bold">3D Event Reconstruction</span>
        [<a href="vera3d/">VERA</a>]
      </li>
      <li>
        <span style="font-weight: bold">Multimodal Question Answering</span>
        [<a href="memexqa/">MemexQA</a>]
      </li>

    </ul>
  </div>



</div>


<!--
	a Junwei Liang's production
	contact: junweiliang1114@gmail.com
-->
</body>
</html>
